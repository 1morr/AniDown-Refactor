"""
AI æ–‡ä»¶é‡å‘½åå™¨æ¨¡å—ã€‚

å®ç° IFileRenamer æ¥å£ï¼Œä½¿ç”¨ AI ç”Ÿæˆæ–‡ä»¶é‡å‘½åæ˜ å°„ã€‚
"""

import json
import logging
from collections import defaultdict
from typing import Any

from src.core.config import config
from src.core.exceptions import (
    AICircuitBreakerError,
    AIKeyExhaustedError,
)
from src.core.interfaces.adapters import IFileRenamer, RenameResult
from src.infrastructure.repositories.ai_key_repository import ai_key_repository
from src.services.ai_debug_service import AIDebugService

from .api_client import OpenAIClient
from .circuit_breaker import CircuitBreaker
from .key_pool import KeyPool
from .prompts import (
    MULTI_FILE_RENAME_STANDARD_PROMPT,
    MULTI_FILE_RENAME_WITH_TVDB_PROMPT,
)
from .schemas import MULTI_FILE_RENAME_RESPONSE_FORMAT

logger = logging.getLogger(__name__)


# é»˜è®¤æ‰¹æ¬¡å¤§å°
DEFAULT_BATCH_SIZE = 30


class AIFileRenamer(IFileRenamer):
    """
    AI æ–‡ä»¶é‡å‘½åå™¨ã€‚

    å®ç° IFileRenamer æ¥å£ï¼Œä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡ä»¶é‡å‘½åæ˜ å°„ã€‚
    æ”¯æŒ TVDB æ•°æ®ã€æ–‡ä»¶å¤¹ç»“æ„è¾“å…¥å’Œæ‰¹é‡å¤„ç†ã€‚

    Example:
        >>> renamer = AIFileRenamer(key_pool, circuit_breaker)
        >>> result = renamer.generate_rename_mapping(
        ...     files=['[ANi] Title - 01 [1080P].mp4'],
        ...     category='tv',
        ...     anime_title='åŠ¨æ¼«æ ‡é¢˜'
        ... )
        >>> if result:
        ...     print(result.main_files)
    """

    # ä»»åŠ¡ç”¨é€”æ ‡è¯†ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼Œç‹¬ç«‹äº Pool åç§°ï¼‰
    TASK_PURPOSE = 'multi_file_rename'

    def __init__(
        self,
        key_pool: KeyPool,
        circuit_breaker: CircuitBreaker,
        api_client: OpenAIClient | None = None,
        max_retries: int = 3,
        batch_size: int = DEFAULT_BATCH_SIZE,
        ai_debug_service: AIDebugService | None = None
    ):
        """
        åˆå§‹åŒ–æ–‡ä»¶é‡å‘½åå™¨ã€‚

        Args:
            key_pool: API Key æ± 
            circuit_breaker: ç†”æ–­å™¨
            api_client: API å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            batch_size: æ‰¹é‡å¤„ç†æ–‡ä»¶æ•°
            ai_debug_service: AI è°ƒè¯•æœåŠ¡ï¼ˆå¯é€‰ï¼‰
        """
        self._key_pool = key_pool
        self._circuit_breaker = circuit_breaker
        self._api_client = api_client or OpenAIClient(timeout=360)
        self._max_retries = max_retries
        self._batch_size = batch_size
        self._ai_debug_service = ai_debug_service

    def generate_rename_mapping(
        self,
        files: list[str],
        category: str,
        anime_title: str | None = None,
        folder_structure: str | None = None,
        tvdb_data: dict[str, Any] | None = None
    ) -> RenameResult | None:
        """
        ç”Ÿæˆæ–‡ä»¶é‡å‘½åæ˜ å°„ã€‚

        æ”¯æŒæ‰¹é‡å¤„ç†å¤§æ–‡ä»¶åˆ—è¡¨ï¼Œè‡ªåŠ¨åˆ†æ‰¹è°ƒç”¨ AIã€‚

        Args:
            files: æ–‡ä»¶ååˆ—è¡¨
            category: å†…å®¹ç±»å‹ ('tv' æˆ– 'movie')
            anime_title: åŠ¨æ¼«æ ‡é¢˜ï¼ˆç”¨äºé‡å‘½åï¼‰
            folder_structure: æ–‡ä»¶å¤¹ç»“æ„ä¿¡æ¯
            tvdb_data: TVDB å…ƒæ•°æ®

        Returns:
            RenameResult: é‡å‘½åç»“æœ
            None: å¤„ç†å¤±è´¥

        Raises:
            AICircuitBreakerError: ç†”æ–­å™¨å·²å¼€å¯
            AIKeyExhaustedError: æ²¡æœ‰å¯ç”¨çš„ API Key
        """
        if not files:
            logger.warning('ğŸ“­ æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†')
            return RenameResult()

        # æ£€æŸ¥ç†”æ–­å™¨æ˜¯å¦å…è®¸è¯·æ±‚
        if not self._circuit_breaker.allow_request():
            remaining = self._circuit_breaker.get_remaining_seconds()
            state = self._circuit_breaker.state.value
            logger.warning(
                f'ğŸ”´ [{self._key_pool.purpose}] ç†”æ–­å™¨çŠ¶æ€: {state}ï¼Œ'
                f'å‰©ä½™ {remaining:.0f}s'
            )
            raise AICircuitBreakerError(
                message=f'ç†”æ–­å™¨çŠ¶æ€: {state}',
                remaining_seconds=remaining
            )

        logger.info(f'ğŸ¤– å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶çš„é‡å‘½å')

        # å¦‚æœæ–‡ä»¶æ•°é‡è¶…è¿‡æ‰¹æ¬¡å¤§å°ï¼Œåˆ†æ‰¹å¤„ç†
        if len(files) > self._batch_size:
            return self._process_batches(
                files=files,
                category=category,
                anime_title=anime_title,
                folder_structure=folder_structure,
                tvdb_data=tvdb_data
            )
        else:
            return self._process_single_batch(
                files=files,
                category=category,
                anime_title=anime_title,
                folder_structure=folder_structure,
                tvdb_data=tvdb_data,
                previous_hardlinks=[]
            )

    def _group_files_by_folder(
        self,
        files: list[str]
    ) -> list[tuple[str, list[str]]]:
        """
        æŒ‰æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç¬¬äºŒå±‚ç›®å½•ï¼‰å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç»„ã€‚

        å°†æ–‡ä»¶æŒ‰å…¶æ‰€å±çš„å­æ–‡ä»¶å¤¹åˆ†ç»„ï¼Œç¡®ä¿åŒä¸€å­£åº¦/å­æ–‡ä»¶å¤¹çš„æ–‡ä»¶
        è¢«æ”¾åœ¨ä¸€èµ·å¤„ç†ï¼Œé¿å…ä¸åŒå­£åº¦çš„æ–‡ä»¶æ··åœ¨åŒä¸€æ‰¹æ¬¡ä¸­ã€‚

        Args:
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶åˆ—è¡¨)
        """
        folder_groups: dict[str, list[str]] = defaultdict(list)

        for file_path in files:
            # è§£ææ–‡ä»¶è·¯å¾„ï¼Œæå–ç¬¬äºŒå±‚ç›®å½•ä½œä¸ºåˆ†ç»„é”®
            # ä¾‹å¦‚: "[VCB-Studio] selector WIXOSS/[VCB-Studio] selector infected WIXOSS [Ma10p_1080p]/..."
            # åˆ†ç»„é”®ä¸º: "[VCB-Studio] selector WIXOSS/[VCB-Studio] selector infected WIXOSS [Ma10p_1080p]"
            parts = file_path.replace('\\', '/').split('/')

            if len(parts) >= 2:
                # ä½¿ç”¨å‰ä¸¤å±‚ç›®å½•ä½œä¸ºåˆ†ç»„é”®
                folder_key = '/'.join(parts[:2])
            elif len(parts) == 1:
                # åªæœ‰æ–‡ä»¶åï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºæ ¹ç›®å½•é”®
                folder_key = ''
            else:
                folder_key = parts[0] if parts else ''

            folder_groups[folder_key].append(file_path)

        # è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨ï¼Œä¿æŒæ–‡ä»¶å¤¹çš„åŸå§‹å‡ºç°é¡ºåº
        seen_folders: list[str] = []
        for file_path in files:
            parts = file_path.replace('\\', '/').split('/')
            if len(parts) >= 2:
                folder_key = '/'.join(parts[:2])
            elif len(parts) == 1:
                folder_key = ''
            else:
                folder_key = parts[0] if parts else ''

            if folder_key not in seen_folders:
                seen_folders.append(folder_key)

        result = [(folder, folder_groups[folder]) for folder in seen_folders]

        logger.debug(
            f'ğŸ“ æ–‡ä»¶æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„: {len(result)} ä¸ªæ–‡ä»¶å¤¹, '
            f'åˆ†åˆ«æœ‰ {[len(g[1]) for g in result]} ä¸ªæ–‡ä»¶'
        )

        return result

    def _process_batches(
        self,
        files: list[str],
        category: str,
        anime_title: str | None,
        folder_structure: str | None,
        tvdb_data: dict[str, Any] | None
    ) -> RenameResult | None:
        """
        åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶åˆ—è¡¨ã€‚

        å…ˆæŒ‰æ–‡ä»¶å¤¹åˆ†ç»„ï¼Œå†å¯¹æ¯ä¸ªæ–‡ä»¶å¤¹ç»„å†…çš„æ–‡ä»¶æŒ‰æ•°é‡åˆ†æ‰¹ã€‚
        ç¡®ä¿ä¸åŒæ–‡ä»¶å¤¹ï¼ˆå¦‚ä¸åŒå­£åº¦ï¼‰çš„æ–‡ä»¶ä¸ä¼šæ··åœ¨åŒä¸€æ‰¹æ¬¡ä¸­ã€‚

        Args:
            files: æ–‡ä»¶ååˆ—è¡¨
            category: å†…å®¹ç±»å‹
            anime_title: åŠ¨æ¼«æ ‡é¢˜
            folder_structure: æ–‡ä»¶å¤¹ç»“æ„
            tvdb_data: TVDB æ•°æ®

        Returns:
            åˆå¹¶åçš„ RenameResult æˆ– None
        """
        merged_result = RenameResult()
        previous_hardlinks: list[str] = []

        # å…ˆæŒ‰æ–‡ä»¶å¤¹åˆ†ç»„
        folder_groups = self._group_files_by_folder(files)

        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
        total_batches = 0
        for folder_name, folder_files in folder_groups:
            folder_batches = (len(folder_files) + self._batch_size - 1) // self._batch_size
            total_batches += folder_batches

        logger.info(
            f'ğŸ“Š åˆ†æ‰¹ç­–ç•¥: {len(folder_groups)} ä¸ªæ–‡ä»¶å¤¹, '
            f'å…± {total_batches} ä¸ªæ‰¹æ¬¡'
        )

        batch_idx = 0
        for folder_name, folder_files in folder_groups:
            folder_display = folder_name.split('/')[-1] if folder_name else 'æ ¹ç›®å½•'
            folder_batch_count = (
                len(folder_files) + self._batch_size - 1
            ) // self._batch_size

            logger.info(
                f'ğŸ“ å¤„ç†æ–‡ä»¶å¤¹ [{folder_display}]: '
                f'{len(folder_files)} ä¸ªæ–‡ä»¶, {folder_batch_count} ä¸ªæ‰¹æ¬¡'
            )

            # å¯¹å½“å‰æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶æŒ‰æ•°é‡åˆ†æ‰¹
            for inner_idx in range(folder_batch_count):
                batch_idx += 1
                start = inner_idx * self._batch_size
                end = min(start + self._batch_size, len(folder_files))
                batch_files = folder_files[start:end]

                logger.info(
                    f'ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches}: '
                    f'{len(batch_files)} ä¸ªæ–‡ä»¶ (æ¥è‡ª {folder_display})'
                )

                batch_result = self._process_single_batch(
                    files=batch_files,
                    category=category,
                    anime_title=anime_title,
                    folder_structure=folder_structure,
                    tvdb_data=tvdb_data,
                    previous_hardlinks=previous_hardlinks
                )

                if batch_result is None:
                    logger.error(f'âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥')
                    continue

                # åˆå¹¶ç»“æœ
                merged_result.main_files.update(batch_result.main_files)
                merged_result.skipped_files.extend(batch_result.skipped_files)

                # åˆå¹¶å­£åº¦ä¿¡æ¯
                for season_key, season_info in batch_result.seasons_info.items():
                    if season_key in merged_result.seasons_info:
                        # ç´¯åŠ é›†æ•°
                        existing = merged_result.seasons_info[season_key]
                        if isinstance(existing, dict) and isinstance(season_info, dict):
                            existing['count'] = (
                                existing.get('count', 0) + season_info.get('count', 0)
                            )
                    else:
                        merged_result.seasons_info[season_key] = season_info

                # æ›´æ–°å·²å¤„ç†çš„ç¡¬é“¾æ¥åˆ—è¡¨ï¼Œç”¨äºåç»­æ‰¹æ¬¡å†²çªæ£€æµ‹
                previous_hardlinks.extend(batch_result.main_files.values())

                # ä¿ç•™æœ€åä¸€æ‰¹çš„ patterns
                merged_result.patterns = batch_result.patterns

        if not merged_result.has_files:
            logger.warning('ğŸ“­ æ‰¹é‡å¤„ç†å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•é‡å‘½åæ˜ å°„')
            return None

        logger.info(
            f'âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {merged_result.file_count} ä¸ªæ–‡ä»¶æ˜ å°„, '
            f'{merged_result.skipped_count} ä¸ªè·³è¿‡'
        )
        return merged_result

    def _process_single_batch(
        self,
        files: list[str],
        category: str,
        anime_title: str | None,
        folder_structure: str | None,
        tvdb_data: dict[str, Any] | None,
        previous_hardlinks: list[str]
    ) -> RenameResult | None:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶ã€‚

        Args:
            files: æ–‡ä»¶ååˆ—è¡¨
            category: å†…å®¹ç±»å‹
            anime_title: åŠ¨æ¼«æ ‡é¢˜
            folder_structure: æ–‡ä»¶å¤¹ç»“æ„
            tvdb_data: TVDB æ•°æ®
            previous_hardlinks: å·²åˆ›å»ºçš„ç¡¬é“¾æ¥åˆ—è¡¨ï¼ˆç”¨äºå†²çªæ£€æµ‹ï¼‰

        Returns:
            RenameResult æˆ– None
        """
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message, indexed_files = self._build_user_message(
            files=files,
            category=category,
            anime_title=anime_title,
            folder_structure=folder_structure,
            tvdb_data=tvdb_data,
            previous_hardlinks=previous_hardlinks
        )

        # é€‰æ‹©æç¤ºè¯
        system_prompt = (
            MULTI_FILE_RENAME_WITH_TVDB_PROMPT
            if tvdb_data
            else MULTI_FILE_RENAME_STANDARD_PROMPT
        )

        for attempt in range(self._max_retries):
            # é¢„ç•™ Keyï¼ˆå¯ç”¨ RPM/RPD ç­‰å¾…ï¼‰
            reservation = self._key_pool.reserve(
                wait_for_rpm=True,
                wait_for_rpd=True
            )
            if not reservation:
                logger.error(f'âŒ [{self._key_pool.purpose}] æ²¡æœ‰å¯ç”¨çš„ API Key')
                raise AIKeyExhaustedError(
                    message='æ²¡æœ‰å¯ç”¨çš„ API Key'
                )

            logger.debug(
                f'ğŸ”‘ å°è¯• {attempt + 1}/{self._max_retries}: '
                f'ä½¿ç”¨ Key {reservation.key_id}'
            )

            # è§£æ extra_bodyï¼ˆä»ä»»åŠ¡é…ç½®è¯»å–ï¼Œä¸æ˜¯ä» poolï¼‰
            extra_params = self._parse_extra_body(config.openai.multi_file_rename.extra_body)

            # è·å–ä»»åŠ¡é…ç½®çš„ modelï¼ˆä¸æ˜¯ä» pool è¯»å–ï¼‰
            model = config.openai.multi_file_rename.model

            # è°ƒç”¨ API
            response = self._api_client.call(
                base_url=reservation.base_url,
                api_key=reservation.api_key,
                model=model,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_message}
                ],
                response_format=MULTI_FILE_RENAME_RESPONSE_FORMAT,
                extra_params=extra_params
            )

            if response.success:
                # æŠ¥å‘ŠæˆåŠŸç»™ Key Pool
                self._key_pool.report_success(
                    reservation.key_id,
                    response_time_ms=response.response_time_ms
                )

                # æŠ¥å‘ŠæˆåŠŸç»™ç†”æ–­å™¨ï¼ˆç”¨äºåŠå¼€çŠ¶æ€æ¢æµ‹ï¼‰
                self._circuit_breaker.report_success()

                # è·å– Key ä¿¡æ¯å’Œå½“å‰ RPM/RPD è®¡æ•°
                pool_status = self._key_pool.get_status()
                key_info = next(
                    (k for k in pool_status.get('keys', []) if k['key_id'] == reservation.key_id),
                    {}
                )

                # è§£æå“åº”
                result = self._parse_response(response.content, indexed_files)

                # è®°å½•åˆ°æ•°æ®åº“
                ai_key_repository.log_usage(
                    purpose=self.TASK_PURPOSE,
                    key_id=reservation.key_id,
                    key_name=key_info.get('name', ''),
                    model=model,
                    anime_title=anime_title or '',
                    context_summary=f'{len(files)} files: {files[0][:50]}...' if files else '',
                    success=True,
                    response_time_ms=response.response_time_ms,
                    rpm_at_call=key_info.get('rpm_count', 0),
                    rpd_at_call=key_info.get('rpd_count', 0),
                )

                # è®°å½• AI è°ƒè¯•æ—¥å¿—
                if self._ai_debug_service and self._ai_debug_service.enabled:
                    self._ai_debug_service.log_ai_interaction(
                        operation='multi_file_rename',
                        input_data={
                            'files': files,
                            'category': category,
                            'anime_title': anime_title,
                            'folder_structure': folder_structure,
                            'tvdb_data': tvdb_data,
                            'previous_hardlinks': previous_hardlinks
                        },
                        output_data=response.content,
                        model=model,
                        response_time_ms=response.response_time_ms,
                        key_id=reservation.key_id,
                        success=True
                    )

                if result:
                    logger.info(
                        f'âœ… æ–‡ä»¶é‡å‘½åæˆåŠŸ: {result.file_count} ä¸ªæ–‡ä»¶ '
                        f'({response.response_time_ms}ms)'
                    )
                    return result
                else:
                    logger.warning('âš ï¸ å“åº”è§£æå¤±è´¥ï¼Œå°è¯•é‡è¯•')
                    continue
            else:
                # è·å– Key ä¿¡æ¯å’Œå½“å‰ RPM/RPD è®¡æ•°
                pool_status = self._key_pool.get_status()
                key_info = next(
                    (k for k in pool_status.get('keys', []) if k['key_id'] == reservation.key_id),
                    {}
                )

                # è®°å½•åˆ°æ•°æ®åº“
                ai_key_repository.log_usage(
                    purpose=self.TASK_PURPOSE,
                    key_id=reservation.key_id,
                    key_name=key_info.get('name', ''),
                    model=model,
                    anime_title=anime_title or '',
                    context_summary=f'{len(files)} files: {files[0][:50]}...' if files else '',
                    success=False,
                    error_code=response.error_code,
                    error_message=response.error_message or 'Unknown error',
                    response_time_ms=response.response_time_ms,
                    rpm_at_call=key_info.get('rpm_count', 0),
                    rpd_at_call=key_info.get('rpd_count', 0),
                )

                # è®°å½• AI è°ƒè¯•æ—¥å¿—ï¼ˆå¤±è´¥ï¼‰
                if self._ai_debug_service and self._ai_debug_service.enabled:
                    self._ai_debug_service.log_ai_interaction(
                        operation='multi_file_rename',
                        input_data={
                            'files': files,
                            'category': category,
                            'anime_title': anime_title
                        },
                        output_data=None,
                        model=model,
                        response_time_ms=response.response_time_ms,
                        key_id=reservation.key_id,
                        success=False,
                        error_message=response.error_message
                    )

                # æŠ¥å‘Šé”™è¯¯ç»™ Key Poolï¼ˆä½¿ç”¨çŠ¶æ€ç åŒºåˆ†é”™è¯¯ç±»å‹ï¼‰
                retry_after = None
                if response.error_code == 429:
                    retry_after = self._extract_retry_after(response.error_message)

                self._key_pool.report_error(
                    reservation.key_id,
                    response.error_message or 'Unknown error',
                    status_code=response.error_code,
                    retry_after=retry_after
                )

                # æŠ¥å‘Šå¤±è´¥ç»™ç†”æ–­å™¨ï¼ˆç”¨äºåŠå¼€çŠ¶æ€æ¢æµ‹ï¼‰
                self._circuit_breaker.report_failure(response.error_message)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘ç†”æ–­
                if pool_status['all_in_long_cooling']:
                    self._circuit_breaker.trip(
                        reason='æ‰€æœ‰ Key éƒ½ä¸å¯ç”¨ï¼ˆé•¿å†·å´æˆ–å·²ç¦ç”¨ï¼‰'
                    )
                    raise AICircuitBreakerError(
                        message='æ‰€æœ‰ Key éƒ½ä¸å¯ç”¨ï¼Œè§¦å‘ç†”æ–­',
                        remaining_seconds=self._circuit_breaker.get_remaining_seconds()
                    )

        logger.error(
            f'âŒ æ–‡ä»¶é‡å‘½åå¤±è´¥: é‡è¯• {self._max_retries} æ¬¡åä»å¤±è´¥'
        )
        return None

    def _build_user_message(
        self,
        files: list[str],
        category: str,
        anime_title: str | None,
        folder_structure: str | None,
        tvdb_data: dict[str, Any] | None,
        previous_hardlinks: list[str]
    ) -> tuple[str, dict[str, str]]:
        """
        æ„å»ºå‘é€ç»™ AI çš„ç”¨æˆ·æ¶ˆæ¯ã€‚

        Args:
            files: æ–‡ä»¶åˆ—è¡¨
            category: å†…å®¹ç±»å‹
            anime_title: åŠ¨æ¼«æ ‡é¢˜
            folder_structure: æ–‡ä»¶å¤¹ç»“æ„
            tvdb_data: TVDB æ•°æ®
            previous_hardlinks: å·²åˆ›å»ºçš„ç¡¬é“¾æ¥

        Returns:
            Tuple[str, Dict[str, str]]: (æ ¼å¼åŒ–çš„ç”¨æˆ·æ¶ˆæ¯, keyåˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„)
        """
        message_parts = []

        # åŸºæœ¬ä¿¡æ¯
        message_parts.append(f'**Category**: {category}')

        if anime_title:
            message_parts.append(f'**Anime Title**: {anime_title}')

        # æ„å»ºå¸¦ key çš„æ–‡ä»¶æ˜ å°„
        indexed_files = {str(i + 1): f for i, f in enumerate(files)}
        files_json = json.dumps({'files': indexed_files}, ensure_ascii=False, indent=2)
        message_parts.append(f'**Files**:\n```json\n{files_json}\n```')

        # æ–‡ä»¶å¤¹ç»“æ„
        if folder_structure:
            message_parts.append(f'**Folder Structure**:\n```\n{folder_structure}\n```')

        # TVDB æ•°æ®
        if tvdb_data:
            tvdb_json = json.dumps(tvdb_data, ensure_ascii=False, indent=2)
            message_parts.append(f'**TVDB Data**:\n```json\n{tvdb_json}\n```')

        # å·²åˆ›å»ºçš„ç¡¬é“¾æ¥ï¼ˆç”¨äºæ‰¹é‡å¤„ç†æ—¶é¿å…å†²çªï¼‰
        if previous_hardlinks:
            hardlinks_json = json.dumps(
                {'previous_hardlinks': previous_hardlinks},
                ensure_ascii=False,
                indent=2
            )
            message_parts.append(
                f'**Previous Hardlinks**:\n```json\n{hardlinks_json}\n```'
            )

        return '\n\n'.join(message_parts), indexed_files

    def _parse_response(
        self,
        content: str | None,
        indexed_files: dict[str, str]
    ) -> RenameResult | None:
        """
        è§£æ AI å“åº”å†…å®¹ã€‚

        Args:
            content: AI å“åº”å†…å®¹
            indexed_files: key åˆ°åŸå§‹æ–‡ä»¶è·¯å¾„çš„æ˜ å°„

        Returns:
            RenameResult æˆ– None
        """
        if not content:
            return None

        try:
            # æ¸…ç† markdown ä»£ç å—
            cleaned = content.strip()
            if cleaned.startswith('```json'):
                cleaned = cleaned[7:]
            if cleaned.startswith('```'):
                cleaned = cleaned[3:]
            if cleaned.endswith('```'):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()

            data = json.loads(cleaned)

            # å°† key è½¬æ¢å›åŸå§‹æ–‡ä»¶è·¯å¾„
            main_files_raw = data.get('main_files', {})
            main_files = {}
            for key, new_name in main_files_raw.items():
                original_path = indexed_files.get(key)
                if original_path:
                    main_files[original_path] = new_name
                else:
                    logger.warning(f'âš ï¸ æœªçŸ¥çš„æ–‡ä»¶ key: {key}')

            # å°† skipped_files çš„ key è½¬æ¢å›åŸå§‹æ–‡ä»¶è·¯å¾„
            skipped_keys = data.get('skipped_files', [])
            skipped_files = []
            for key in skipped_keys:
                original_path = indexed_files.get(key)
                if original_path:
                    skipped_files.append(original_path)
                else:
                    logger.warning(f'âš ï¸ æœªçŸ¥çš„è·³è¿‡æ–‡ä»¶ key: {key}')

            seasons_info = data.get('seasons_info', {})

            # æ„å»º patterns ä¿¡æ¯
            patterns = {
                'subtitle_group_regex': data.get('subtitle_group_regex', ''),
                'full_title_regex': data.get('full_title_regex', ''),
                'clean_title_regex': data.get('clean_title_regex', ''),
                'episode_regex': data.get('episode_regex', ''),
                'special_tag_regex': data.get('special_tag_regex', ''),
                'quality_regex': data.get('quality_regex', ''),
                'platform_regex': data.get('platform_regex', ''),
                'source_regex': data.get('source_regex', ''),
                'codec_regex': data.get('codec_regex', ''),
                'subtitle_type_regex': data.get('subtitle_type_regex', ''),
                'format_regex': data.get('format_regex', ''),
                'anime_full_title': data.get('anime_full_title', ''),
                'anime_clean_title': data.get('anime_clean_title', ''),
                'subtitle_group_name': data.get('subtitle_group_name', ''),
                'season': data.get('season', 1),
                'category': data.get('category', 'tv'),
                'method': 'ai'
            }

            return RenameResult(
                main_files=main_files,
                skipped_files=skipped_files,
                seasons_info=seasons_info,
                patterns=patterns,
                method='ai'
            )

        except json.JSONDecodeError as e:
            logger.error(f'âŒ JSON è§£æå¤±è´¥: {e}')
            logger.debug(f'å“åº”å†…å®¹: {content[:500]}')
            return None

        except (KeyError, TypeError, ValueError) as e:
            logger.error(f'âŒ æ•°æ®æå–å¤±è´¥: {e}')
            return None

        except Exception as e:
            logger.exception(f'âŒ å“åº”è§£ææœªé¢„æœŸé”™è¯¯: {e}')
            return None

    def _extract_retry_after(self, error_message: str | None) -> float | None:
        """
        ä»é”™è¯¯æ¶ˆæ¯ä¸­æå– retry-after æ—¶é—´ã€‚

        Args:
            error_message: é”™è¯¯æ¶ˆæ¯

        Returns:
            é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰æˆ– None
        """
        if not error_message:
            return None

        import re

        # å°è¯•åŒ¹é…å¸¸è§çš„ retry-after æ ¼å¼
        patterns = [
            r'retry.?after[:\s]+(\d+(?:\.\d+)?)\s*(?:s|seconds?)?',
            r'wait[:\s]+(\d+(?:\.\d+)?)\s*(?:s|seconds?)?',
            r'(\d+(?:\.\d+)?)\s*(?:s|seconds?)\s*(?:before|until)',
        ]

        for pattern in patterns:
            match = re.search(pattern, error_message, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue

        return None

    def _parse_extra_body(self, extra_body: str) -> dict[str, Any] | None:
        """
        è§£æ extra_body JSON å­—ç¬¦ä¸²ã€‚

        Args:
            extra_body: JSON æ ¼å¼çš„é¢å¤–å‚æ•°å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„å­—å…¸ï¼Œè§£æå¤±è´¥æˆ–ä¸ºç©ºåˆ™è¿”å› None
        """
        if not extra_body or not extra_body.strip():
            return None

        try:
            parsed = json.loads(extra_body)
            if isinstance(parsed, dict) and parsed:
                logger.debug(f'ğŸ”§ ä½¿ç”¨ extra_body å‚æ•°: {list(parsed.keys())}')
                return parsed
            return None
        except json.JSONDecodeError as e:
            logger.warning(f'âš ï¸ extra_body JSON è§£æå¤±è´¥: {e}')
            return None
