"""
AniDown Application Entry Point.

This module serves as the main entry point for the AniDown application.
It initializes the core infrastructure, starts the web server, webhook handler,
and RSS scheduler.
"""

import argparse
import base64
import logging
import os
import sys
import time
import schedule
from datetime import datetime, timedelta, timezone
from threading import Thread
from typing import Optional

# è¨­ç½®æ—¥èªŒè·¯å¾‘
log_path = os.getenv('LOG_PATH', 'logs')
os.makedirs(log_path, exist_ok=True)

# ç”Ÿæˆå¸¶æ—¥æœŸçš„æ—¥èªŒæ–‡ä»¶å
today = datetime.now().strftime('%Y-%m-%d')
log_file = os.path.join(log_path, f'anidown_{today}.log')

# é…ç½®æ—¥å¿— - ä¿®å¾© Windows æ§åˆ¶å° UTF-8 ç·¨ç¢¼å•é¡Œ
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setStream(open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        stream_handler
    ]
)
logger = logging.getLogger(__name__)


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    from src.infrastructure.database.session import db_manager

    logger.info('ğŸ’¾ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...')
    db_manager.init_db()
    logger.info('âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ')


def init_key_pools():
    """åˆå§‹åŒ– API Key Pool å’Œç†”æ–­å™¨"""
    from src.core.config import config
    from src.container import container
    from src.infrastructure.ai.key_pool import KeySpec, register_pool
    from src.infrastructure.ai.circuit_breaker import register_breaker

    # åˆå§‹åŒ– title_parse key pool
    title_parse_pool = container.title_parse_pool()
    title_parse_breaker = container.title_parse_breaker()
    title_parse_config = config.openai.title_parse

    keys = []
    # ä¼˜å…ˆä½¿ç”¨ api_key_pool
    if title_parse_config.api_key_pool:
        for idx, key_entry in enumerate(title_parse_config.api_key_pool):
            if key_entry.enabled and key_entry.api_key:
                keys.append(KeySpec(
                    key_id=f'tp_key_{idx}',
                    name=key_entry.name or f'Key {idx + 1}',
                    api_key=key_entry.api_key,
                    base_url=title_parse_config.base_url,
                    model=title_parse_config.model,
                    rpm_limit=key_entry.rpm,
                    rpd_limit=key_entry.rpd,
                    enabled=True
                ))
    # å›é€€åˆ°å•ä¸ª api_key
    elif title_parse_config.api_key:
        keys.append(KeySpec(
            key_id='tp_key_0',
            name='Primary Key',
            api_key=title_parse_config.api_key,
            base_url=title_parse_config.base_url,
            model=title_parse_config.model,
            rpm_limit=0,
            rpd_limit=0,
            enabled=True
        ))

    if keys:
        title_parse_pool.configure(keys)
        register_pool(title_parse_pool)
        register_breaker(title_parse_breaker)
        logger.info(f'ğŸ”‘ Title Parse Key Pool å·²é…ç½®: {len(keys)} ä¸ª Key')
    else:
        logger.warning('âš ï¸ Title Parse æœªé…ç½® API Key')

    # åˆå§‹åŒ– multi_file_rename key pool
    rename_pool = container.rename_pool()
    rename_breaker = container.rename_breaker()
    rename_config = config.openai.multi_file_rename

    rename_keys = []
    if rename_config.api_key_pool:
        for idx, key_entry in enumerate(rename_config.api_key_pool):
            if key_entry.enabled and key_entry.api_key:
                rename_keys.append(KeySpec(
                    key_id=f'rn_key_{idx}',
                    name=key_entry.name or f'Key {idx + 1}',
                    api_key=key_entry.api_key,
                    base_url=rename_config.base_url,
                    model=rename_config.model,
                    rpm_limit=key_entry.rpm,
                    rpd_limit=key_entry.rpd,
                    enabled=True
                ))
    elif rename_config.api_key:
        rename_keys.append(KeySpec(
            key_id='rn_key_0',
            name='Primary Key',
            api_key=rename_config.api_key,
            base_url=rename_config.base_url,
            model=rename_config.model,
            rpm_limit=0,
            rpd_limit=0,
            enabled=True
        ))

    if rename_keys:
        rename_pool.configure(rename_keys)
        register_pool(rename_pool)
        register_breaker(rename_breaker)
        logger.info(f'ğŸ”‘ Rename Key Pool å·²é…ç½®: {len(rename_keys)} ä¸ª Key')
    else:
        logger.warning('âš ï¸ Multi-File Rename æœªé…ç½® API Key')


def init_discord_webhook():
    """åˆå§‹åŒ– Discord Webhook å®¢æˆ·ç«¯"""
    from src.core.config import config
    from src.container import container

    discord_client = container.discord_webhook()

    # æ„å»º webhook URL æ˜ å°„
    webhooks = {}
    if config.discord.rss_webhook_url:
        webhooks['rss'] = config.discord.rss_webhook_url
    if config.discord.hardlink_webhook_url:
        webhooks['hardlink'] = config.discord.hardlink_webhook_url
        # ä¸‹è½½å®Œæˆé€šçŸ¥ä¹Ÿä½¿ç”¨ hardlink webhook
        webhooks['download'] = config.discord.hardlink_webhook_url

    # é…ç½® webhook å®¢æˆ·ç«¯
    discord_client.configure(
        webhooks=webhooks,
        enabled=config.discord.enabled
    )

    if config.discord.enabled and webhooks:
        logger.info(f'ğŸ”” Discord é€šçŸ¥å·²å¯ç”¨: {list(webhooks.keys())}')
    elif not config.discord.enabled:
        logger.info('ğŸ”• Discord é€šçŸ¥å·²ç¦ç”¨')
    else:
        logger.warning('âš ï¸ Discord å·²å¯ç”¨ä½†æœªé…ç½® Webhook URL')


def test_config():
    """æµ‹è¯•é…ç½®æ¨¡å—"""
    from src.core.config import config

    logger.info('ğŸ“‹ æµ‹è¯•é…ç½®æ¨¡å—...')
    logger.info(f'  qBittorrent URL: {config.qbittorrent.url}')
    logger.info(f'  RSS æ£€æŸ¥é—´éš”: {config.rss.check_interval}ç§’')
    logger.info(f'  Webhook ç«¯å£: {config.webhook.port}')
    logger.info(f'  WebUI ç«¯å£: {config.webui.port}')
    logger.info('âœ… é…ç½®æ¨¡å—æµ‹è¯•é€šè¿‡')


def test_repositories():
    """æµ‹è¯•ä»“å‚¨æ¨¡å—"""
    from src.infrastructure.repositories.anime_repository import AnimeRepository
    from src.infrastructure.repositories.download_repository import DownloadRepository
    from src.infrastructure.repositories.history_repository import HistoryRepository

    logger.info('ğŸ—„ï¸ æµ‹è¯•ä»“å‚¨æ¨¡å—...')

    anime_repo = AnimeRepository()
    download_repo = DownloadRepository()
    history_repo = HistoryRepository()

    anime_count = anime_repo.count_all()
    download_count = download_repo.count_all()
    hardlink_count = history_repo.count_hardlinks()

    logger.info(f'  åŠ¨æ¼«æ•°é‡: {anime_count}')
    logger.info(f'  ä¸‹è½½æ•°é‡: {download_count}')
    logger.info(f'  ç¡¬é“¾æ¥æ•°é‡: {hardlink_count}')
    logger.info('âœ… ä»“å‚¨æ¨¡å—æµ‹è¯•é€šè¿‡')


def test_qbit_adapter():
    """æµ‹è¯• qBittorrent é€‚é…å™¨"""
    from src.infrastructure.downloader.qbit_adapter import QBitAdapter

    logger.info('ğŸ“¥ æµ‹è¯• qBittorrent é€‚é…å™¨...')

    qb = QBitAdapter()
    if qb.is_connected():
        logger.info('âœ… qBittorrent è¿æ¥æˆåŠŸ')

        # è·å–ç§å­åˆ—è¡¨
        torrents = qb.get_all_torrents()
        if torrents is not None:
            logger.info(f'  æ´»åŠ¨ç§å­æ•°: {len(torrents)}')
    else:
        logger.warning('âš ï¸ qBittorrent è¿æ¥å¤±è´¥ (å¯èƒ½æœªé…ç½®æˆ–æœåŠ¡æœªå¯åŠ¨)')


def test_container():
    """æµ‹è¯•ä¾èµ–æ³¨å…¥å®¹å™¨"""
    from src.container import container

    logger.info('ğŸ“¦ æµ‹è¯•ä¾èµ–æ³¨å…¥å®¹å™¨...')

    # æµ‹è¯•è·å–å„ä¸ªç»„ä»¶
    anime_repo = container.anime_repo()
    download_repo = container.download_repo()
    history_repo = container.history_repo()
    qb_client = container.qb_client()

    logger.info('  âœ“ AnimeRepository')
    logger.info('  âœ“ DownloadRepository')
    logger.info('  âœ“ HistoryRepository')
    logger.info('  âœ“ QBitAdapter')

    # æµ‹è¯•æ–°å¢çš„æœåŠ¡
    try:
        title_parser = container.title_parser()
        logger.info('  âœ“ AITitleParser')
    except Exception as e:
        logger.warning(f'  âš  AITitleParser (å¯èƒ½æœªé…ç½®API Key): {e}')

    try:
        download_manager = container.download_manager()
        logger.info('  âœ“ DownloadManager')
    except Exception as e:
        logger.warning(f'  âš  DownloadManager: {e}')

    logger.info('âœ… ä¾èµ–æ³¨å…¥å®¹å™¨æµ‹è¯•é€šè¿‡')


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info('ğŸš€ AniDown éªŒè¯æµ‹è¯•å¼€å§‹...')
    logger.info(f'ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {os.getenv("CONFIG_PATH", "config.json")}')
    logger.info(f'ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}')
    logger.info('')

    try:
        test_config()
        logger.info('')

        init_database()
        logger.info('')

        test_repositories()
        logger.info('')

        test_qbit_adapter()
        logger.info('')

        test_container()
        logger.info('')

        logger.info('=' * 50)
        logger.info('ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼')
        logger.info('=' * 50)
        return True

    except Exception as e:
        logger.error(f'âŒ éªŒè¯å¤±è´¥: {e}', exc_info=True)
        return False


def init_queue_workers(download_manager):
    """
    åˆå§‹åŒ–é˜Ÿåˆ—å·¥ä½œè€…å¹¶æ³¨å†Œå¤„ç†å™¨ã€‚

    Args:
        download_manager: DownloadManager å®ä¾‹
    """
    from src.services.queue.webhook_queue import get_webhook_queue, WebhookQueueWorker
    from src.services.queue.rss_queue import get_rss_queue, RSSQueueWorker

    # åˆå§‹åŒ– Webhook é˜Ÿåˆ—
    webhook_queue = get_webhook_queue()

    def handle_torrent_completed(payload):
        """å¤„ç†ç§å­å®Œæˆäº‹ä»¶"""
        try:
            logger.info(f'ğŸ”” å¤„ç†ç§å­å®Œæˆäº‹ä»¶: {payload.hash_id[:8]}...')
            download_manager.handle_torrent_completed(payload.hash_id)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­å®Œæˆäº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_added(payload):
        """å¤„ç†ç§å­æ·»åŠ äº‹ä»¶"""
        try:
            logger.info(f'ğŸ“¥ ç§å­å·²æ·»åŠ : {payload.name}')
            download_manager.handle_torrent_added(payload.hash_id)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­æ·»åŠ äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_error(payload):
        """å¤„ç†ç§å­é”™è¯¯äº‹ä»¶"""
        try:
            logger.warning(f'âš ï¸ ç§å­é”™è¯¯: {payload.name}')
            download_manager.handle_torrent_error(
                payload.hash_id,
                payload.extra_data.get('error', 'æœªçŸ¥é”™è¯¯')
            )
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­é”™è¯¯äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_paused(payload):
        """å¤„ç†ç§å­æš‚åœäº‹ä»¶"""
        try:
            logger.info(f'â¸ï¸ ç§å­å·²æš‚åœ: {payload.name}')
            download_manager.handle_torrent_paused(payload.hash_id)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­æš‚åœäº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    # æ³¨å†Œ Webhook å¤„ç†å™¨
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_COMPLETED,
        handle_torrent_completed
    )
    # å…¼å®¹ qBittorrent çš„ torrent_finished äº‹ä»¶
    webhook_queue.register_handler(
        'torrent_finished',
        handle_torrent_completed
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_ADDED,
        handle_torrent_added
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_ERROR,
        handle_torrent_error
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_PAUSED,
        handle_torrent_paused
    )

    # å¯åŠ¨ Webhook é˜Ÿåˆ—
    webhook_queue.start()
    logger.info('âœ… Webhook é˜Ÿåˆ— worker å·²å¯åŠ¨')

    # åˆå§‹åŒ– RSS é˜Ÿåˆ—
    rss_queue = get_rss_queue()

    def handle_rss_event(payload):
        """å¤„ç† RSS Feed äº‹ä»¶ - è§£æ Feed å¹¶å°†é¡¹ç›®åŠ å…¥é˜Ÿåˆ—"""
        try:
            from src.core.config import RSSFeed
            from src.container import container

            # ä» extra_data è·å–å®Œæ•´çš„ feed é…ç½®
            feed_data = payload.extra_data.get('feed_data', {})

            # ä¼˜å…ˆä½¿ç”¨ feed_dataï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» extra_data æ ¹å±‚çº§è·å–
            blocked_keywords = feed_data.get('blocked_keywords', '') or payload.extra_data.get('blocked_keywords', '')
            blocked_regex = feed_data.get('blocked_regex', '') or payload.extra_data.get('blocked_regex', '')
            media_type = feed_data.get('media_type', '') or payload.extra_data.get('media_type', 'anime')

            logger.info(f'ğŸ“¡ è§£æ RSS Feed: {payload.rss_url[:50]}...')

            # ä»å®¹å™¨è·å–æœåŠ¡
            rss_service = container.rss_service()
            history_repo = container.history_repo()
            download_repo = container.download_repo()

            # åˆ›å»ºå†å²è®°å½•
            history_id = history_repo.insert_rss_history(
                rss_url=payload.rss_url,
                triggered_by=payload.trigger_type
            )

            # è§£æ RSS Feed
            items = rss_service.parse_feed(payload.rss_url)

            if not items:
                logger.info(f'ğŸ“­ RSS Feed æ²¡æœ‰æ–°é¡¹ç›®: {payload.rss_url[:50]}...')
                history_repo.update_rss_history_stats(
                    history_id,
                    items_found=0,
                    items_attempted=0,
                    items_processed=0,
                    status='completed'
                )
                return

            logger.info(f'ğŸ“¥ å‘ç° {len(items)} ä¸ªé¡¹ç›®ï¼Œæ­£åœ¨è¿‡æ»¤å’ŒåŠ å…¥é˜Ÿåˆ—...')

            # è¿‡æ»¤å™¨é…ç½®
            filter_config = {
                'blocked_keywords': blocked_keywords,
                'blocked_regex': blocked_regex,
            }

            # å°†æ¯ä¸ªé¡¹ç›®åŠ å…¥é˜Ÿåˆ—
            enqueued_count = 0
            filtered_count = 0
            exists_count = 0
            filter_service = container.filter_service()

            for item in items:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if item.hash:
                    existing = download_repo.get_by_hash(item.hash)
                    if existing:
                        history_repo.insert_rss_detail(
                            history_id, item.title, 'exists', 'å·²å­˜åœ¨äºæ•°æ®åº“'
                        )
                        exists_count += 1
                        continue

                # æ£€æŸ¥è¿‡æ»¤å™¨
                if blocked_keywords or blocked_regex:
                    if filter_service.should_filter(item.title, blocked_keywords, blocked_regex):
                        logger.info(f'â­ï¸ è¿‡æ»¤è·³è¿‡: {item.title[:50]}...')
                        history_repo.insert_rss_detail(
                            history_id, item.title, 'filtered', 'åŒ¹é…è¿‡æ»¤è§„åˆ™'
                        )
                        filtered_count += 1
                        continue

                # åŠ å…¥é˜Ÿåˆ—
                rss_queue.enqueue_single_item(
                    item_title=item.title,
                    torrent_url=item.torrent_url or item.link,
                    hash_id=item.hash or '',
                    rss_url=payload.rss_url,
                    media_type=media_type,
                    extra_data={
                        'trigger_type': payload.trigger_type,
                        'description': item.description,
                        'pub_date': item.pub_date,
                        'history_id': history_id,
                        **filter_config
                    }
                )
                enqueued_count += 1

            # æ›´æ–°å†å²è®°å½•ç»Ÿè®¡
            history_repo.update_rss_history_stats(
                history_id,
                items_found=len(items),
                items_attempted=enqueued_count,
                status='processing' if enqueued_count > 0 else 'completed'
            )

            logger.info(
                f'âœ… RSSå¤„ç†å®Œæˆ: æ€»æ•°={len(items)}, '
                f'å·²å­˜åœ¨={exists_count}, è¿‡æ»¤={filtered_count}, '
                f'åŠ å…¥é˜Ÿåˆ—={enqueued_count}'
            )

        except Exception as e:
            logger.error(f'âŒ å¤„ç† RSS Feed äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_single_item(payload):
        """å¤„ç†å•ä¸ª RSS é¡¹ç›®"""
        try:
            from src.container import container

            logger.info(f'ğŸ”„ å¤„ç†é¡¹ç›®: {payload.item_title[:50]}...')

            # è·å– history_idï¼ˆå¦‚æœæœ‰ï¼‰
            history_id = payload.extra_data.get('history_id')
            history_repo = container.history_repo() if history_id else None

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            download_repo = container.download_repo()

            if payload.hash_id:
                existing = download_repo.get_by_hash(payload.hash_id)
                if existing:
                    logger.info(f'â­ï¸ é¡¹ç›®å·²å­˜åœ¨: {payload.item_title[:50]}...')
                    if history_repo and history_id:
                        history_repo.insert_rss_detail(
                            history_id, payload.item_title, 'exists', 'å·²å­˜åœ¨äºæ•°æ®åº“'
                        )
                    return

            # è°ƒç”¨ DownloadManager å¤„ç†å•ä¸ªé¡¹ç›®
            item_data = {
                'title': payload.item_title,
                'torrent_url': payload.torrent_url,
                'hash': payload.hash_id,
                'link': payload.torrent_url,
                'media_type': payload.media_type,
                'description': payload.extra_data.get('description', ''),
                'pub_date': payload.extra_data.get('pub_date'),
            }

            success = download_manager.process_single_rss_item(
                item_data,
                payload.extra_data.get('trigger_type', 'queue')
            )

            # è®°å½•å¤„ç†ç»“æœ
            if history_repo and history_id:
                if success:
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'success'
                    )
                    # æ›´æ–°å¤„ç†è®¡æ•°
                    history_repo.increment_rss_history_processed(history_id)
                else:
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'failed', 'å¤„ç†å¤±è´¥'
                    )

            if success:
                logger.info(f'âœ… é¡¹ç›®å¤„ç†æˆåŠŸ: {payload.item_title[:50]}...')
            else:
                logger.warning(f'âš ï¸ é¡¹ç›®å¤„ç†å¤±è´¥: {payload.item_title[:50]}...')

        except Exception as e:
            logger.error(f'âŒ å¤„ç†å•ä¸ªé¡¹ç›®å¤±è´¥: {e}', exc_info=True)
            # è®°å½•å¤±è´¥
            try:
                if history_id:
                    from src.container import container
                    history_repo = container.history_repo()
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'failed', str(e)
                    )
            except Exception:
                pass

    # æ³¨å†Œ RSS å¤„ç†å™¨
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SCHEDULED_CHECK,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_MANUAL_CHECK,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SINGLE_FEED,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_FIXED_SUBSCRIPTION,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SINGLE_ITEM,
        handle_single_item
    )

    # å¯åŠ¨ RSS é˜Ÿåˆ—
    rss_queue.start()
    logger.info('âœ… RSS é˜Ÿåˆ— worker å·²å¯åŠ¨')

    return webhook_queue, rss_queue


def run_schedule(download_manager):
    """
    è¿è¡Œå®šæ—¶ä»»åŠ¡ã€‚

    Args:
        download_manager: DownloadManager å®ä¾‹
    """
    from src.core.config import config
    from src.interface.web.controllers.system_status import system_status_manager
    from src.services.queue.rss_queue import get_rss_queue, RSSQueueWorker, RSSPayload

    logger.info('ğŸ”” å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...')
    logger.info(f'ğŸ“‹ RSSæ£€æŸ¥é—´éš”: {config.rss.check_interval} ç§’')

    rss_feeds = config.rss.get_feeds()
    logger.info(f'ğŸ“¡ å·²é…ç½® {len(rss_feeds)} ä¸ªRSSè®¢é˜…æº')

    # æ ‡è®° RSS è°ƒåº¦å™¨ä¸ºè¿è¡Œä¸­
    system_status_manager.set_rss_scheduler_status(True)

    # è·å– RSS é˜Ÿåˆ— worker
    rss_queue = get_rss_queue()

    def enqueue_rss_feeds(triggered_by: str):
        """å°†æ‰€æœ‰é…ç½®çš„ RSS feeds åŠ å…¥é˜Ÿåˆ—"""
        feeds = config.rss.get_feeds()
        if not feeds:
            logger.info('ğŸ“­ æ²¡æœ‰é…ç½®RSSé“¾æ¥')
            return

        for feed in feeds:
            feed_data = {
                'url': feed.url,
                'blocked_keywords': feed.blocked_keywords,
                'blocked_regex': feed.blocked_regex,
                'media_type': feed.media_type,
            }
            payload = RSSPayload(
                rss_url=feed.url,
                trigger_type=triggered_by,
                extra_data={'feed_data': feed_data}
            )
            rss_queue.enqueue_event(
                event_type=RSSQueueWorker.EVENT_SINGLE_FEED,
                payload=payload
            )
        logger.info(f'ğŸ“¥ å·²å°† {len(feeds)} ä¸ªRSSé“¾æ¥åŠ å…¥å¤„ç†é˜Ÿåˆ—')

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    logger.info('âš¡ ç«‹å³æ‰§è¡Œé¦–æ¬¡RSSæ£€æŸ¥...')
    enqueue_rss_feeds('å¯åŠ¨æ—¶è§¦å‘')

    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    def scheduled_task():
        enqueue_rss_feeds('å®šæ—¶è§¦å‘')
        next_run = datetime.now() + timedelta(seconds=config.rss.check_interval)
        logger.info(f"â° ä¸‹æ¬¡RSSæ£€æŸ¥æ—¶é—´: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")

    schedule.every(config.rss.check_interval).seconds.do(scheduled_task)

    while True:
        schedule.run_pending()
        time.sleep(1)


def handle_rss_command(args, download_manager):
    """
    å¤„ç†RSSå‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    from src.core.config import RSSFeed

    logger.info(f'ğŸ”„ å¤„ç†RSSé“¾æ¥: {args.url}')
    feed = RSSFeed(url=args.url)
    download_manager.process_rss_feeds([feed], 'å‘½ä»¤è¡Œè§¦å‘')


def handle_magnet_command(args, download_manager):
    """
    å¤„ç†ç£åŠ›é“¾æ¥å‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    logger.info(f'ğŸ”„ å¤„ç†ç£åŠ›é“¾æ¥: {args.hash}')
    magnet_link = f'magnet:?xt=urn:btih:{args.hash}'
    data = {
        'upload_type': 'magnet',
        'magnet_link': magnet_link,
        'anime_title': args.title,
        'subtitle_group': args.group,
        'season': args.season,
        'category': args.category
    }
    success = download_manager.process_manual_upload(data)
    if success:
        logger.info('âœ… ç£åŠ›é“¾æ¥å¤„ç†æˆåŠŸ')
    else:
        logger.error('âŒ ç£åŠ›é“¾æ¥å¤„ç†å¤±è´¥')


def handle_torrent_command(args, download_manager):
    """
    å¤„ç†Torrentæ–‡ä»¶å‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    logger.info(f'ğŸ”„ å¤„ç†Torrentæ–‡ä»¶: {args.file}')

    try:
        with open(args.file, 'rb') as f:
            torrent_content = base64.b64encode(f.read()).decode('utf-8')

        data = {
            'upload_type': 'torrent',
            'torrent_file': torrent_content,
            'anime_title': args.title,
            'subtitle_group': args.group,
            'season': args.season,
            'category': args.category
        }
        success = download_manager.process_manual_upload(data)
        if success:
            logger.info('âœ… Torrentæ–‡ä»¶å¤„ç†æˆåŠŸ')
        else:
            logger.error('âŒ Torrentæ–‡ä»¶å¤„ç†å¤±è´¥')
    except Exception as e:
        logger.error(f'âŒ è¯»å–Torrentæ–‡ä»¶å¤±è´¥: {e}')


def start_webhook_server(host: str, port: int):
    """
    å¯åŠ¨ Webhook æœåŠ¡å™¨ã€‚

    Args:
        host: ç›‘å¬åœ°å€
        port: ç›‘å¬ç«¯å£
    """
    from flask import Flask
    from src.interface.webhook.handler import create_webhook_blueprint

    app = Flask(__name__)
    app.register_blueprint(create_webhook_blueprint())

    # ä½¿ç”¨ Werkzeug é™é»˜æ¨¡å¼
    import logging as werkzeug_logging
    werkzeug_logging.getLogger('werkzeug').setLevel(werkzeug_logging.WARNING)

    app.run(host=host, port=port, debug=False, use_reloader=False)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    from src.core.config import config
    from src.container import container
    from src.services.ai_debug_service import ai_debug_service

    parser = argparse.ArgumentParser(description='AniDown - åŠ¨æ¼«ä¸‹è½½ç®¡ç†å™¨')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨debugæ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='è¿è¡ŒéªŒè¯æµ‹è¯•')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # RSSå‘½ä»¤
    rss_parser = subparsers.add_parser('rss', help='å¤„ç†RSSé“¾æ¥')
    rss_parser.add_argument('url', help='RSSé“¾æ¥')

    # ç£åŠ›é“¾æ¥å‘½ä»¤
    magnet_parser = subparsers.add_parser('magnet', help='å¤„ç†ç£åŠ›é“¾æ¥')
    magnet_parser.add_argument('hash', help='ç£åŠ›é“¾æ¥hash')
    magnet_parser.add_argument('title', help='åŠ¨æ¼«åç§°')
    magnet_parser.add_argument('group', help='å­—å¹•ç»„')
    magnet_parser.add_argument('--season', type=int, default=1, help='å­£æ•°')
    magnet_parser.add_argument('--category', default='tv', choices=['tv', 'movie'])

    # Torrentæ–‡ä»¶å‘½ä»¤
    torrent_parser = subparsers.add_parser('torrent', help='å¤„ç†Torrentæ–‡ä»¶')
    torrent_parser.add_argument('file', help='Torrentæ–‡ä»¶è·¯å¾„')
    torrent_parser.add_argument('title', help='åŠ¨æ¼«åç§°')
    torrent_parser.add_argument('group', help='å­—å¹•ç»„')
    torrent_parser.add_argument('--season', type=int, default=1, help='å­£æ•°')
    torrent_parser.add_argument('--category', default='tv', choices=['tv', 'movie'])

    args = parser.parse_args()

    # å¯ç”¨debugæ¨¡å¼
    if args.debug:
        ai_debug_service.enable()
        logger.info('ğŸ› DEBUGæ¨¡å¼å·²å¯ç”¨')
        logging.getLogger().setLevel(logging.DEBUG)

    # éªŒè¯æµ‹è¯•æ¨¡å¼
    if args.test:
        success = run_all_tests()
        sys.exit(0 if success else 1)

    logger.info('ğŸš€ AniDown å¯åŠ¨ä¸­...')
    logger.info(f'ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {os.getenv("CONFIG_PATH", "config.json")}')

    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()

    # åˆå§‹åŒ– Discord Webhook
    init_discord_webhook()

    # åˆå§‹åŒ– API Key Pool
    init_key_pools()

    # è·å– DownloadManager å®ä¾‹
    download_manager = container.download_manager()

    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if args.command == 'rss':
        handle_rss_command(args, download_manager)
        return
    elif args.command == 'magnet':
        handle_magnet_command(args, download_manager)
        return
    elif args.command == 'torrent':
        handle_torrent_command(args, download_manager)
        return

    # é»˜è®¤å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼
    logger.info('ğŸ¬ å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼...')

    # å¯¼å…¥çŠ¶æ€ç®¡ç†å™¨
    from src.interface.web.controllers.system_status import system_status_manager

    # åˆå§‹åŒ–é˜Ÿåˆ—å·¥ä½œè€…
    webhook_queue, rss_queue = init_queue_workers(download_manager)

    # å¯åŠ¨ Webhook æœåŠ¡å™¨ (åå°çº¿ç¨‹)
    if config.webhook.enabled:
        logger.info(f'ğŸ”— æ­£åœ¨å¯åŠ¨ Webhook æœåŠ¡å™¨...')
        logger.info(f'ğŸ“ Webhook åœ°å€: http://{config.webhook.host}:{config.webhook.port}')
        webhook_thread = Thread(
            target=start_webhook_server,
            kwargs={'host': config.webhook.host, 'port': config.webhook.port},
            daemon=True
        )
        webhook_thread.start()
        system_status_manager.set_webhook_status(True)
        logger.info('âœ… Webhook æœåŠ¡å™¨å·²åœ¨åå°å¯åŠ¨')
    else:
        logger.info('â­ï¸ Webhook æœåŠ¡å™¨å·²ç¦ç”¨')

    # å¯åŠ¨ Web UI æœåŠ¡å™¨ (åå°çº¿ç¨‹)
    if config.webui.enabled:
        logger.info(f'ğŸŒ æ­£åœ¨å¯åŠ¨ Web UI æœåŠ¡å™¨...')

        def run_webui():
            from src.interface.web.app import create_app

            # ä½¿ç”¨ Werkzeug é™é»˜æ¨¡å¼
            import logging as werkzeug_logging
            werkzeug_logging.getLogger('werkzeug').setLevel(werkzeug_logging.WARNING)

            app = create_app(container)
            system_status_manager.set_webui_status(True)
            app.run(
                host=config.webui.host,
                port=config.webui.port,
                debug=False,
                use_reloader=False
            )

        webui_thread = Thread(target=run_webui, daemon=True)
        webui_thread.start()
        logger.info(f'âœ… Web UI æœåŠ¡å™¨å·²å¯åŠ¨: http://{config.webui.host}:{config.webui.port}')
    else:
        logger.info('â­ï¸ Web UI æœåŠ¡å™¨å·²ç¦ç”¨')

    # å¯åŠ¨å®šæ—¶ä»»åŠ¡ (ä¸»çº¿ç¨‹)
    try:
        run_schedule(download_manager)
    except KeyboardInterrupt:
        logger.info('ğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...')
        system_status_manager.set_rss_scheduler_status(False)
        system_status_manager.set_webui_status(False)
        system_status_manager.set_webhook_status(False)

        # åœæ­¢é˜Ÿåˆ—å·¥ä½œè€…
        webhook_queue.stop()
        rss_queue.stop()
        logger.info('âœ… å·²ä¼˜é›…å…³é—­')
    except Exception as e:
        logger.error(f'âŒ å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}', exc_info=True)
        system_status_manager.set_rss_scheduler_status(False)


if __name__ == '__main__':
    main()
