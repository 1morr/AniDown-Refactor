"""
AniDown Application Entry Point.

This module serves as the main entry point for the AniDown application.
It initializes the core infrastructure, starts the web server, webhook handler,
and RSS scheduler.
"""

import argparse
import base64
import logging
import os
import sys
import time
import schedule
from datetime import datetime, timedelta, timezone
from threading import Thread
from typing import Optional

# è¨­ç½®æ—¥èªŒè·¯å¾‘
log_path = os.getenv('LOG_PATH', 'logs')
os.makedirs(log_path, exist_ok=True)

# ç”Ÿæˆå¸¶æ—¥æœŸçš„æ—¥èªŒæ–‡ä»¶å
today = datetime.now().strftime('%Y-%m-%d')
log_file = os.path.join(log_path, f'anidown_{today}.log')

# é…ç½®æ—¥å¿— - ä¿®å¾© Windows æ§åˆ¶å° UTF-8 ç·¨ç¢¼å•é¡Œ
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setStream(open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        stream_handler
    ]
)
logger = logging.getLogger(__name__)


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    from src.infrastructure.database.session import db_manager

    logger.info('ğŸ’¾ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...')
    db_manager.init_db()
    logger.info('âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ')


def init_key_pools():
    """åˆå§‹åŒ– API Key Pool å’Œç†”æ–­å™¨"""
    from dependency_injector import providers

    from src.core.config import config
    from src.container import container
    from src.infrastructure.ai.key_pool import (
        KeyPool, KeySpec,
        register_pool, register_named_pool, bind_purpose_to_pool,
        get_named_pool, clear_all_registries
    )
    from src.infrastructure.ai.circuit_breaker import (
        CircuitBreaker,
        register_breaker, register_named_breaker, get_named_breaker,
        clear_all_breaker_registries
    )

    # æ¸…ç©ºç°æœ‰æ³¨å†Œè¡¨ï¼ˆæ”¯æŒé…ç½®çƒ­é‡è½½ï¼‰
    clear_all_registries()
    clear_all_breaker_registries()

    # Phase 1: åˆ›å»ºå‘½å Key Poolsï¼ˆä» config.openai.key_poolsï¼‰
    for pool_def in config.openai.key_pools:
        pool_name = pool_def.name
        if not pool_name:
            logger.warning('âš ï¸ Key Pool ç¼ºå°‘åç§°ï¼Œè·³è¿‡')
            continue

        # åˆ›å»º KeyPool å’Œ CircuitBreaker
        pool = KeyPool(purpose=f'pool:{pool_name}')
        breaker = CircuitBreaker(purpose=f'pool:{pool_name}')

        # è½¬æ¢é…ç½®ä¸­çš„ keys
        keys = []
        for idx, key_entry in enumerate(pool_def.api_keys):
            if key_entry.enabled and key_entry.api_key:
                keys.append(KeySpec(
                    key_id=f'{pool_name}_key_{idx}',
                    name=key_entry.name or f'Key {idx + 1}',
                    api_key=key_entry.api_key,
                    base_url=pool_def.base_url,
                    rpm_limit=key_entry.rpm,
                    rpd_limit=key_entry.rpd,
                    enabled=True,
                    extra_body=''  # extra_body åœ¨ä»»åŠ¡çº§åˆ«è®¾ç½®ï¼Œä¸åœ¨ pool ä¸­
                ))

        if keys:
            pool.configure(keys)
            register_named_pool(pool, pool_name)
            register_named_breaker(breaker, pool_name)
            pool.restore_counts_from_db()
            logger.info(f'ğŸ”‘ å‘½å Key Pool "{pool_name}" å·²é…ç½®: {len(keys)} ä¸ª Key')
        else:
            logger.warning(f'âš ï¸ å‘½å Key Pool "{pool_name}" æ²¡æœ‰æœ‰æ•ˆçš„ API Key')

    # Phase 2: ä¸ºæ¯ä¸ªä»»åŠ¡ç»‘å®šæ± æˆ–åˆ›å»ºç‹¬ç«‹æ± 
    # ä½¿ç”¨ provider å¼•ç”¨ä»¥ä¾¿å¯ä»¥ override
    task_configs = [
        ('title_parse', config.openai.title_parse, container.title_parse_pool, container.title_parse_breaker),
        ('multi_file_rename', config.openai.multi_file_rename, container.rename_pool, container.rename_breaker),
        ('subtitle_match', config.openai.subtitle_match, container.subtitle_match_pool, container.subtitle_match_breaker),
    ]

    for purpose, task_config, pool_provider, breaker_provider in task_configs:
        if task_config.pool_name:
            # ä½¿ç”¨å‘½å Pool - è·å–å·²åˆ›å»ºçš„å…±äº«å®ä¾‹
            named_pool = get_named_pool(task_config.pool_name)
            named_breaker = get_named_breaker(task_config.pool_name)

            if named_pool and named_breaker:
                # è¦†ç›–å®¹å™¨ providerï¼Œä½¿å…¶è¿”å›å…±äº«å®ä¾‹
                pool_provider.override(providers.Object(named_pool))
                breaker_provider.override(providers.Object(named_breaker))

                # ç»‘å®šä»»åŠ¡ç”¨é€”åˆ° pool åç§°
                bind_purpose_to_pool(purpose, task_config.pool_name)

                # åŒæ—¶æ³¨å†Œåˆ°ç”¨é€”æ³¨å†Œè¡¨ï¼ˆç”¨äº API æŸ¥æ‰¾ï¼‰
                register_pool(named_pool)
                register_breaker(named_breaker)

                logger.info(
                    f'ğŸ”— ä»»åŠ¡ {purpose} å…±äº« Pool "{task_config.pool_name}"'
                )
            else:
                logger.warning(
                    f'âš ï¸ ä»»åŠ¡ {purpose} å¼•ç”¨çš„ Pool "{task_config.pool_name}" ä¸å­˜åœ¨'
                )
        elif task_config.api_key:
            # ä½¿ç”¨ç‹¬ç«‹é…ç½®ï¼ˆå•ä¸ª API Keyï¼‰
            pool = pool_provider()
            breaker = breaker_provider()

            keys = [KeySpec(
                key_id=f'{purpose}_key_0',
                name='Primary Key',
                api_key=task_config.api_key,
                base_url=task_config.base_url,
                rpm_limit=0,
                rpd_limit=0,
                enabled=True,
                extra_body=task_config.extra_body
            )]

            pool.configure(keys)
            register_pool(pool)
            register_breaker(breaker)
            pool.restore_counts_from_db()
            logger.info(f'ğŸ”‘ ä»»åŠ¡ {purpose} ä½¿ç”¨ç‹¬ç«‹é…ç½®: 1 ä¸ª Key')
        else:
            logger.warning(f'âš ï¸ ä»»åŠ¡ {purpose} æœªé…ç½® API Key æˆ– Key Pool')


def init_discord_webhook():
    """åˆå§‹åŒ– Discord Webhook å®¢æˆ·ç«¯"""
    from src.core.config import config
    from src.container import container

    discord_client = container.discord_webhook()

    # æ„å»º webhook URL æ˜ å°„
    webhooks = {}
    if config.discord.rss_webhook_url:
        webhooks['rss'] = config.discord.rss_webhook_url
    if config.discord.hardlink_webhook_url:
        webhooks['hardlink'] = config.discord.hardlink_webhook_url
        # ä¸‹è½½å®Œæˆé€šçŸ¥ä¹Ÿä½¿ç”¨ hardlink webhook
        webhooks['download'] = config.discord.hardlink_webhook_url

    # é…ç½® webhook å®¢æˆ·ç«¯
    discord_client.configure(
        webhooks=webhooks,
        enabled=config.discord.enabled
    )

    if config.discord.enabled and webhooks:
        logger.info(f'ğŸ”” Discord é€šçŸ¥å·²å¯ç”¨: {list(webhooks.keys())}')
    elif not config.discord.enabled:
        logger.info('ğŸ”• Discord é€šçŸ¥å·²ç¦ç”¨')
    else:
        logger.warning('âš ï¸ Discord å·²å¯ç”¨ä½†æœªé…ç½® Webhook URL')


def test_config():
    """æµ‹è¯•é…ç½®æ¨¡å—"""
    from src.core.config import config

    logger.info('ğŸ“‹ æµ‹è¯•é…ç½®æ¨¡å—...')
    logger.info(f'  qBittorrent URL: {config.qbittorrent.url}')
    logger.info(f'  RSS æ£€æŸ¥é—´éš”: {config.rss.check_interval}ç§’')
    logger.info(f'  Webhook ç«¯å£: {config.webhook.port}')
    logger.info(f'  WebUI ç«¯å£: {config.webui.port}')
    logger.info('âœ… é…ç½®æ¨¡å—æµ‹è¯•é€šè¿‡')


def test_repositories():
    """æµ‹è¯•ä»“å‚¨æ¨¡å—"""
    from src.infrastructure.repositories.anime_repository import AnimeRepository
    from src.infrastructure.repositories.download_repository import DownloadRepository
    from src.infrastructure.repositories.history_repository import HistoryRepository

    logger.info('ğŸ—„ï¸ æµ‹è¯•ä»“å‚¨æ¨¡å—...')

    anime_repo = AnimeRepository()
    download_repo = DownloadRepository()
    history_repo = HistoryRepository()

    anime_count = anime_repo.count_all()
    download_count = download_repo.count_all()
    hardlink_count = history_repo.count_hardlinks()

    logger.info(f'  åŠ¨æ¼«æ•°é‡: {anime_count}')
    logger.info(f'  ä¸‹è½½æ•°é‡: {download_count}')
    logger.info(f'  ç¡¬é“¾æ¥æ•°é‡: {hardlink_count}')
    logger.info('âœ… ä»“å‚¨æ¨¡å—æµ‹è¯•é€šè¿‡')


def test_qbit_adapter():
    """æµ‹è¯• qBittorrent é€‚é…å™¨"""
    from src.infrastructure.downloader.qbit_adapter import QBitAdapter

    logger.info('ğŸ“¥ æµ‹è¯• qBittorrent é€‚é…å™¨...')

    qb = QBitAdapter()
    if qb.is_connected():
        logger.info('âœ… qBittorrent è¿æ¥æˆåŠŸ')

        # è·å–ç§å­åˆ—è¡¨
        torrents = qb.get_all_torrents()
        if torrents is not None:
            logger.info(f'  æ´»åŠ¨ç§å­æ•°: {len(torrents)}')
    else:
        logger.warning('âš ï¸ qBittorrent è¿æ¥å¤±è´¥ (å¯èƒ½æœªé…ç½®æˆ–æœåŠ¡æœªå¯åŠ¨)')


def test_container():
    """æµ‹è¯•ä¾èµ–æ³¨å…¥å®¹å™¨"""
    from src.container import container

    logger.info('ğŸ“¦ æµ‹è¯•ä¾èµ–æ³¨å…¥å®¹å™¨...')

    # æµ‹è¯•è·å–å„ä¸ªç»„ä»¶
    anime_repo = container.anime_repo()
    download_repo = container.download_repo()
    history_repo = container.history_repo()
    qb_client = container.qb_client()

    logger.info('  âœ“ AnimeRepository')
    logger.info('  âœ“ DownloadRepository')
    logger.info('  âœ“ HistoryRepository')
    logger.info('  âœ“ QBitAdapter')

    # æµ‹è¯•æ–°å¢çš„æœåŠ¡
    try:
        title_parser = container.title_parser()
        logger.info('  âœ“ AITitleParser')
    except Exception as e:
        logger.warning(f'  âš  AITitleParser (å¯èƒ½æœªé…ç½®API Key): {e}')

    try:
        download_manager = container.download_manager()
        logger.info('  âœ“ DownloadManager')
    except Exception as e:
        logger.warning(f'  âš  DownloadManager: {e}')

    logger.info('âœ… ä¾èµ–æ³¨å…¥å®¹å™¨æµ‹è¯•é€šè¿‡')


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info('ğŸš€ AniDown éªŒè¯æµ‹è¯•å¼€å§‹...')
    logger.info(f'ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {os.getenv("CONFIG_PATH", "config.json")}')
    logger.info(f'ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}')
    logger.info('')

    try:
        test_config()
        logger.info('')

        init_database()
        logger.info('')

        test_repositories()
        logger.info('')

        test_qbit_adapter()
        logger.info('')

        test_container()
        logger.info('')

        logger.info('=' * 50)
        logger.info('ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼')
        logger.info('=' * 50)
        return True

    except Exception as e:
        logger.error(f'âŒ éªŒè¯å¤±è´¥: {e}', exc_info=True)
        return False


def init_queue_workers(download_manager):
    """
    åˆå§‹åŒ–é˜Ÿåˆ—å·¥ä½œè€…å¹¶æ³¨å†Œå¤„ç†å™¨ã€‚

    Args:
        download_manager: DownloadManager å®ä¾‹
    """
    from src.services.queue.webhook_queue import get_webhook_queue, WebhookQueueWorker
    from src.services.queue.rss_queue import get_rss_queue, RSSQueueWorker

    # åˆå§‹åŒ– Webhook é˜Ÿåˆ—
    webhook_queue = get_webhook_queue()

    def handle_torrent_completed(payload):
        """å¤„ç†ç§å­å®Œæˆäº‹ä»¶"""
        try:
            logger.info(f'ğŸ”” å¤„ç†ç§å­å®Œæˆäº‹ä»¶: {payload.hash_id[:8]}...')
            # æ„å»º webhook_data å­—å…¸ï¼Œä¼ é€’ payload ä¸­çš„æ‰€æœ‰ä¿¡æ¯
            webhook_data = {
                'name': payload.name,
                'save_path': payload.save_path,
                'content_path': payload.extra_data.get('content_path', '') if payload.extra_data else '',
                'category': payload.category,
                'status': payload.status,
            }
            download_manager.handle_torrent_completed(payload.hash_id, webhook_data)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­å®Œæˆäº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_added(payload):
        """å¤„ç†ç§å­æ·»åŠ äº‹ä»¶"""
        try:
            logger.info(f'ğŸ“¥ ç§å­å·²æ·»åŠ : {payload.name}')
            download_manager.handle_torrent_added(payload.hash_id)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­æ·»åŠ äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_error(payload):
        """å¤„ç†ç§å­é”™è¯¯äº‹ä»¶"""
        try:
            logger.warning(f'âš ï¸ ç§å­é”™è¯¯: {payload.name}')
            download_manager.handle_torrent_error(
                payload.hash_id,
                payload.extra_data.get('error', 'æœªçŸ¥é”™è¯¯')
            )
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­é”™è¯¯äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_torrent_paused(payload):
        """å¤„ç†ç§å­æš‚åœäº‹ä»¶"""
        try:
            logger.info(f'â¸ï¸ ç§å­å·²æš‚åœ: {payload.name}')
            download_manager.handle_torrent_paused(payload.hash_id)
        except Exception as e:
            logger.error(f'âŒ å¤„ç†ç§å­æš‚åœäº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    # æ³¨å†Œ Webhook å¤„ç†å™¨
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_COMPLETED,
        handle_torrent_completed
    )
    # å…¼å®¹ qBittorrent çš„ torrent_finished äº‹ä»¶
    webhook_queue.register_handler(
        'torrent_finished',
        handle_torrent_completed
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_ADDED,
        handle_torrent_added
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_ERROR,
        handle_torrent_error
    )
    webhook_queue.register_handler(
        WebhookQueueWorker.EVENT_TORRENT_PAUSED,
        handle_torrent_paused
    )

    # å¯åŠ¨ Webhook é˜Ÿåˆ—
    webhook_queue.start()
    logger.info('âœ… Webhook é˜Ÿåˆ— worker å·²å¯åŠ¨')

    # åˆå§‹åŒ– RSS é˜Ÿåˆ—
    rss_queue = get_rss_queue()

    def handle_rss_event(payload):
        """å¤„ç† RSS Feed äº‹ä»¶ - è§£æ Feed å¹¶å°†é¡¹ç›®åŠ å…¥é˜Ÿåˆ—"""
        try:
            from src.core.config import RSSFeed
            from src.container import container
            from src.core.interfaces.notifications import RSSNotification

            # ä» extra_data è·å–å®Œæ•´çš„ feed é…ç½®
            feed_data = payload.extra_data.get('feed_data', {})

            # ä¼˜å…ˆä½¿ç”¨ feed_dataï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» extra_data æ ¹å±‚çº§è·å–
            blocked_keywords = feed_data.get('blocked_keywords', '') or payload.extra_data.get('blocked_keywords', '')
            blocked_regex = feed_data.get('blocked_regex', '') or payload.extra_data.get('blocked_regex', '')
            media_type = feed_data.get('media_type', '') or payload.extra_data.get('media_type', 'anime')

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹å¤„ç†æ¨¡å¼
            batch_history_id = payload.extra_data.get('batch_history_id')
            is_batch_mode = batch_history_id is not None

            logger.info(f'ğŸ“¡ è§£æ RSS Feed: {payload.rss_url[:50]}...')

            # ä»å®¹å™¨è·å–æœåŠ¡
            rss_service = container.rss_service()
            history_repo = container.history_repo()
            download_repo = container.download_repo()
            rss_notifier = container.rss_notifier()

            # å‘é€ RSS å¼€å§‹é€šçŸ¥ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ä¸‹è·³è¿‡ï¼Œé¿å…é€šçŸ¥è¿‡å¤šï¼‰
            if not is_batch_mode:
                try:
                    rss_notifier.notify_processing_start(
                        RSSNotification(
                            trigger_type=payload.trigger_type,
                            rss_url=payload.rss_url
                        )
                    )
                except Exception as e:
                    logger.warning(f'âš ï¸ å‘é€RSSå¼€å§‹é€šçŸ¥å¤±è´¥: {e}')

            # ä½¿ç”¨æ‰¹å¤„ç†å†å²IDæˆ–åˆ›å»ºæ–°çš„å†å²è®°å½•
            if is_batch_mode:
                history_id = batch_history_id
            else:
                history_id = history_repo.insert_rss_history(
                    rss_url=payload.rss_url,
                    triggered_by=payload.trigger_type
                )

            # è§£æ RSS Feed
            items = rss_service.parse_feed(payload.rss_url)

            if not items:
                logger.info(f'ğŸ“­ RSS Feed æ²¡æœ‰æ–°é¡¹ç›®: {payload.rss_url[:50]}...')
                # æ‰¹å¤„ç†æ¨¡å¼ä¸‹ç´¯åŠ ç»Ÿè®¡ï¼Œå•ç‹¬æ¨¡å¼ä¸‹ç›´æ¥è®¾ç½®
                if is_batch_mode:
                    history_repo.accumulate_rss_history_stats(
                        history_id,
                        items_found=0,
                        items_attempted=0
                    )
                else:
                    history_repo.update_rss_history_stats(
                        history_id,
                        items_found=0,
                        items_attempted=0,
                        items_processed=0,
                        status='completed'
                    )
                    # å‘é€å®Œæˆé€šçŸ¥ï¼ˆæ— é¡¹ç›®ï¼‰
                    try:
                        rss_notifier.notify_processing_complete(
                            success_count=0,
                            total_count=0,
                            failed_items=[],
                            attempt_count=0,
                            status='completed'
                        )
                    except Exception as e:
                        logger.warning(f'âš ï¸ å‘é€RSSå®Œæˆé€šçŸ¥å¤±è´¥: {e}')
                return

            logger.info(f'ğŸ“¥ å‘ç° {len(items)} ä¸ªé¡¹ç›®ï¼Œæ­£åœ¨è¿‡æ»¤å’ŒåŠ å…¥é˜Ÿåˆ—...')

            # è¿‡æ»¤å™¨é…ç½®
            filter_config = {
                'blocked_keywords': blocked_keywords,
                'blocked_regex': blocked_regex,
            }

            # å°†æ¯ä¸ªé¡¹ç›®åŠ å…¥é˜Ÿåˆ—
            enqueued_count = 0
            filtered_count = 0
            exists_count = 0
            filter_service = container.filter_service()

            for item in items:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if item.hash:
                    existing = download_repo.get_by_hash(item.hash)
                    if existing:
                        history_repo.insert_rss_detail(
                            history_id, item.title, 'exists', 'å·²å­˜åœ¨äºæ•°æ®åº“'
                        )
                        exists_count += 1
                        continue

                # æ£€æŸ¥è¿‡æ»¤å™¨
                if blocked_keywords or blocked_regex:
                    if filter_service.should_filter(item.title, blocked_keywords, blocked_regex):
                        logger.info(f'â­ï¸ è¿‡æ»¤è·³è¿‡: {item.title[:50]}...')
                        history_repo.insert_rss_detail(
                            history_id, item.title, 'filtered', 'åŒ¹é…è¿‡æ»¤è§„åˆ™'
                        )
                        filtered_count += 1
                        continue

                # åŠ å…¥é˜Ÿåˆ—
                rss_queue.enqueue_single_item(
                    item_title=item.title,
                    torrent_url=item.torrent_url or item.link,
                    hash_id=item.hash or '',
                    rss_url=payload.rss_url,
                    media_type=media_type,
                    extra_data={
                        'trigger_type': payload.trigger_type,
                        'description': item.description,
                        'pub_date': item.pub_date,
                        'history_id': history_id,
                        **filter_config
                    }
                )
                enqueued_count += 1

            # æ›´æ–°å†å²è®°å½•ç»Ÿè®¡
            if is_batch_mode:
                # æ‰¹å¤„ç†æ¨¡å¼ï¼šç´¯åŠ ç»Ÿè®¡
                history_repo.accumulate_rss_history_stats(
                    history_id,
                    items_found=len(items),
                    items_attempted=enqueued_count
                )
            else:
                # å•ç‹¬æ¨¡å¼ï¼šç›´æ¥è®¾ç½®
                history_repo.update_rss_history_stats(
                    history_id,
                    items_found=len(items),
                    items_attempted=enqueued_count,
                    status='processing' if enqueued_count > 0 else 'completed'
                )

            logger.info(
                f'âœ… RSSå¤„ç†å®Œæˆ: æ€»æ•°={len(items)}, '
                f'å·²å­˜åœ¨={exists_count}, è¿‡æ»¤={filtered_count}, '
                f'åŠ å…¥é˜Ÿåˆ—={enqueued_count}'
            )

            # å¦‚æœæ²¡æœ‰é¡¹ç›®åŠ å…¥é˜Ÿåˆ—ä¸”éæ‰¹å¤„ç†æ¨¡å¼ï¼Œå‘é€å®Œæˆé€šçŸ¥
            if enqueued_count == 0 and not is_batch_mode:
                try:
                    rss_notifier.notify_processing_complete(
                        success_count=0,
                        total_count=len(items),
                        failed_items=[],
                        attempt_count=0,
                        status='completed'
                    )
                except Exception as e:
                    logger.warning(f'âš ï¸ å‘é€RSSå®Œæˆé€šçŸ¥å¤±è´¥: {e}')

            # æ‰¹å¤„ç†æ¨¡å¼ï¼šè·Ÿè¸ªå·²å¤„ç†çš„feedæ•°é‡
            if is_batch_mode:
                batch_total = payload.extra_data.get('batch_total', 1)

                # é€’å¢å·²å¤„ç†çš„feedè®¡æ•°
                history_repo.increment_batch_feeds_processed(history_id)

                # è·å–å½“å‰çŠ¶æ€
                stats = history_repo.get_rss_history_stats(history_id)
                feeds_processed = stats.get('batch_feeds_processed', 0) if stats else 0
                total_attempted = stats.get('items_attempted', 0) if stats else 0
                total_found = stats.get('items_found', 0) if stats else 0

                logger.debug(
                    f'ğŸ“Š æ‰¹å¤„ç†è¿›åº¦: feeds={feeds_processed}/{batch_total}, '
                    f'items_attempted={total_attempted}'
                )

                # å¦‚æœæ‰€æœ‰feedéƒ½å¤„ç†å®Œæˆä¸”æ²¡æœ‰é¡¹ç›®éœ€è¦å¤„ç†ï¼ˆå…¨éƒ¨è¿‡æ»¤/å­˜åœ¨ï¼‰
                if feeds_processed >= batch_total and total_attempted == 0:
                    # æ‰€æœ‰é¡¹ç›®éƒ½è¢«è¿‡æ»¤æˆ–å·²å­˜åœ¨ï¼Œå‘é€å®Œæˆé€šçŸ¥
                    history_repo.update_rss_history_stats(
                        history_id,
                        status='completed'
                    )
                    try:
                        rss_notifier.notify_processing_complete(
                            success_count=0,
                            total_count=total_found,
                            failed_items=[],
                            attempt_count=0,
                            status='completed'
                        )
                    except Exception as e:
                        logger.warning(f'âš ï¸ å‘é€æ‰¹å¤„ç†å®Œæˆé€šçŸ¥å¤±è´¥: {e}')

        except Exception as e:
            logger.error(f'âŒ å¤„ç† RSS Feed äº‹ä»¶å¤±è´¥: {e}', exc_info=True)

    def handle_single_item(payload):
        """å¤„ç†å•ä¸ª RSS é¡¹ç›®"""
        try:
            from src.container import container

            logger.info(f'ğŸ”„ å¤„ç†é¡¹ç›®: {payload.item_title[:50]}...')

            # è·å– history_idï¼ˆå¦‚æœæœ‰ï¼‰
            history_id = payload.extra_data.get('history_id')
            history_repo = container.history_repo() if history_id else None

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            download_repo = container.download_repo()

            if payload.hash_id:
                existing = download_repo.get_by_hash(payload.hash_id)
                if existing:
                    logger.info(f'â­ï¸ é¡¹ç›®å·²å­˜åœ¨: {payload.item_title[:50]}...')
                    if history_repo and history_id:
                        history_repo.insert_rss_detail(
                            history_id, payload.item_title, 'exists', 'å·²å­˜åœ¨äºæ•°æ®åº“'
                        )
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªé¡¹ç›®
                        _check_and_send_rss_completion(history_repo, history_id)
                    return

            # è°ƒç”¨ DownloadManager å¤„ç†å•ä¸ªé¡¹ç›®
            item_data = {
                'title': payload.item_title,
                'torrent_url': payload.torrent_url,
                'hash': payload.hash_id,
                'link': payload.torrent_url,
                'media_type': payload.media_type,
                'description': payload.extra_data.get('description', ''),
                'pub_date': payload.extra_data.get('pub_date'),
            }

            success = download_manager.process_single_rss_item(
                item_data,
                payload.extra_data.get('trigger_type', 'queue')
            )

            # è®°å½•å¤„ç†ç»“æœ
            if history_repo and history_id:
                if success:
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'success'
                    )
                    # æ›´æ–°å¤„ç†è®¡æ•°
                    history_repo.increment_rss_history_processed(history_id)
                else:
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'failed', 'å¤„ç†å¤±è´¥'
                    )

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªé¡¹ç›®ï¼Œå‘é€å®Œæˆé€šçŸ¥
                _check_and_send_rss_completion(history_repo, history_id)

            if success:
                logger.info(f'âœ… é¡¹ç›®å¤„ç†æˆåŠŸ: {payload.item_title[:50]}...')
            else:
                logger.warning(f'âš ï¸ é¡¹ç›®å¤„ç†å¤±è´¥: {payload.item_title[:50]}...')

        except Exception as e:
            logger.error(f'âŒ å¤„ç†å•ä¸ªé¡¹ç›®å¤±è´¥: {e}', exc_info=True)
            # è®°å½•å¤±è´¥
            try:
                if history_id:
                    from src.container import container
                    history_repo = container.history_repo()
                    history_repo.insert_rss_detail(
                        history_id, payload.item_title, 'failed', str(e)
                    )
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªé¡¹ç›®
                    _check_and_send_rss_completion(history_repo, history_id)
            except Exception:
                pass

    def _check_and_send_rss_completion(history_repo, history_id):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¡¹ç›®å·²å¤„ç†å®Œæˆï¼Œå¦‚æœæ˜¯åˆ™å‘é€å®Œæˆé€šçŸ¥"""
        try:
            from src.container import container

            # è·å–å†å²è®°å½•ç»Ÿè®¡
            stats = history_repo.get_rss_history_stats(history_id)
            if not stats:
                return

            items_attempted = stats.get('items_attempted', 0)
            items_processed = stats.get('items_processed', 0)
            status = stats.get('status', 'processing')

            # è·å–è¯¦ç»†ç»Ÿè®¡
            detail_stats = history_repo.get_rss_detail_stats(history_id)
            success_count = detail_stats.get('success', 0)
            failed_count = detail_stats.get('failed', 0)
            exists_count = detail_stats.get('exists', 0)
            filtered_count = detail_stats.get('filtered', 0)

            # è®¡ç®—å·²å¤„ç†çš„é¡¹ç›®æ•°ï¼ˆåªè®¡ç®—æˆåŠŸå’Œå¤±è´¥ï¼Œä¸åŒ…æ‹¬å­˜åœ¨/è¿‡æ»¤çš„ï¼‰
            # å› ä¸º items_attempted åªåŒ…å«å®é™…å…¥é˜Ÿçš„é¡¹ç›®ï¼Œä¸åŒ…æ‹¬ç›´æ¥æ ‡è®°ä¸º exists/filtered çš„
            actual_processed = success_count + failed_count

            logger.debug(
                f'ğŸ“Š RSSæ‰¹æ¬¡è¿›åº¦: å®é™…å¤„ç†={actual_processed}, å°è¯•={items_attempted}, '
                f'æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, å·²å­˜åœ¨={exists_count}'
            )

            # å¦‚æœæ‰€æœ‰å…¥é˜Ÿé¡¹ç›®éƒ½å¤„ç†å®Œæˆï¼Œå‘é€å®Œæˆé€šçŸ¥
            if actual_processed >= items_attempted and items_attempted > 0:
                # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                history_repo.update_rss_history_stats(
                    history_id,
                    items_processed=success_count,
                    status='completed'
                )

                # å‘é€å®Œæˆé€šçŸ¥
                rss_notifier = container.rss_notifier()
                items_found = stats.get('items_found', items_attempted)

                # æ„å»ºå¤±è´¥é¡¹ç›®åˆ—è¡¨
                failed_items = []
                if failed_count > 0:
                    failed_details = history_repo.get_rss_details_by_status(history_id, 'failed')
                    for detail in failed_details[:5]:  # æœ€å¤š5ä¸ª
                        failed_items.append({
                            'title': detail.get('item_title', ''),
                            'reason': detail.get('error_message', 'å¤„ç†å¤±è´¥')
                        })

                # ç¡®å®šçŠ¶æ€
                if failed_count > 0 and success_count == 0:
                    final_status = 'failed'
                elif failed_count > 0:
                    final_status = 'partial'
                else:
                    final_status = 'completed'

                logger.info(f'ğŸ“¤ å‘é€RSSå®Œæˆé€šçŸ¥: æˆåŠŸ={success_count}, æ€»æ•°={items_found}')
                rss_notifier.notify_processing_complete(
                    success_count=success_count,
                    total_count=items_found,
                    failed_items=failed_items,
                    attempt_count=items_attempted,
                    status=final_status
                )

        except Exception as e:
            logger.warning(f'âš ï¸ æ£€æŸ¥RSSå®ŒæˆçŠ¶æ€å¤±è´¥: {e}')

    # æ³¨å†Œ RSS å¤„ç†å™¨
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SCHEDULED_CHECK,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_MANUAL_CHECK,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SINGLE_FEED,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_FIXED_SUBSCRIPTION,
        handle_rss_event
    )
    rss_queue.register_handler(
        RSSQueueWorker.EVENT_SINGLE_ITEM,
        handle_single_item
    )

    # å¯åŠ¨ RSS é˜Ÿåˆ—
    rss_queue.start()
    logger.info('âœ… RSS é˜Ÿåˆ— worker å·²å¯åŠ¨')

    return webhook_queue, rss_queue


def run_schedule(download_manager):
    """
    è¿è¡Œå®šæ—¶ä»»åŠ¡ã€‚

    Args:
        download_manager: DownloadManager å®ä¾‹
    """
    from src.core.config import config
    from src.interface.web.controllers.system_status import system_status_manager
    from src.services.queue.rss_queue import get_rss_queue, RSSQueueWorker, RSSPayload

    logger.info('ğŸ”” å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...')
    logger.info(f'ğŸ“‹ RSSæ£€æŸ¥é—´éš”: {config.rss.check_interval} ç§’')

    rss_feeds = config.rss.get_feeds()
    logger.info(f'ğŸ“¡ å·²é…ç½® {len(rss_feeds)} ä¸ªRSSè®¢é˜…æº')

    # æ ‡è®° RSS è°ƒåº¦å™¨ä¸ºè¿è¡Œä¸­
    system_status_manager.set_rss_scheduler_status(True)

    # è·å– RSS é˜Ÿåˆ— worker
    rss_queue = get_rss_queue()

    def enqueue_rss_feeds(triggered_by: str):
        """å°†æ‰€æœ‰é…ç½®çš„ RSS feeds åŠ å…¥é˜Ÿåˆ—"""
        feeds = config.rss.get_feeds()
        if not feeds:
            logger.info('ğŸ“­ æ²¡æœ‰é…ç½®RSSé“¾æ¥')
            return

        for feed in feeds:
            feed_data = {
                'url': feed.url,
                'blocked_keywords': feed.blocked_keywords,
                'blocked_regex': feed.blocked_regex,
                'media_type': feed.media_type,
            }
            payload = RSSPayload(
                rss_url=feed.url,
                trigger_type=triggered_by,
                extra_data={'feed_data': feed_data}
            )
            rss_queue.enqueue_event(
                event_type=RSSQueueWorker.EVENT_SINGLE_FEED,
                payload=payload
            )
        logger.info(f'ğŸ“¥ å·²å°† {len(feeds)} ä¸ªRSSé“¾æ¥åŠ å…¥å¤„ç†é˜Ÿåˆ—')

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    logger.info('âš¡ ç«‹å³æ‰§è¡Œé¦–æ¬¡RSSæ£€æŸ¥...')
    enqueue_rss_feeds('å¯åŠ¨æ—¶è§¦å‘')

    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    def scheduled_task():
        enqueue_rss_feeds('å®šæ—¶è§¦å‘')
        next_run = datetime.now() + timedelta(seconds=config.rss.check_interval)
        logger.info(f"â° ä¸‹æ¬¡RSSæ£€æŸ¥æ—¶é—´: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")

    schedule.every(config.rss.check_interval).seconds.do(scheduled_task)

    while True:
        schedule.run_pending()
        time.sleep(1)


def handle_rss_command(args, download_manager):
    """
    å¤„ç†RSSå‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    from src.core.config import RSSFeed

    logger.info(f'ğŸ”„ å¤„ç†RSSé“¾æ¥: {args.url}')
    feed = RSSFeed(url=args.url)
    download_manager.process_rss_feeds([feed], 'å‘½ä»¤è¡Œè§¦å‘')


def handle_magnet_command(args, download_manager):
    """
    å¤„ç†ç£åŠ›é“¾æ¥å‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    logger.info(f'ğŸ”„ å¤„ç†ç£åŠ›é“¾æ¥: {args.hash}')
    magnet_link = f'magnet:?xt=urn:btih:{args.hash}'
    data = {
        'upload_type': 'magnet',
        'magnet_link': magnet_link,
        'anime_title': args.title,
        'subtitle_group': args.group,
        'season': args.season,
        'category': args.category
    }
    success = download_manager.process_manual_upload(data)
    if success:
        logger.info('âœ… ç£åŠ›é“¾æ¥å¤„ç†æˆåŠŸ')
    else:
        logger.error('âŒ ç£åŠ›é“¾æ¥å¤„ç†å¤±è´¥')


def handle_torrent_command(args, download_manager):
    """
    å¤„ç†Torrentæ–‡ä»¶å‘½ä»¤ã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        download_manager: DownloadManager å®ä¾‹
    """
    logger.info(f'ğŸ”„ å¤„ç†Torrentæ–‡ä»¶: {args.file}')

    try:
        with open(args.file, 'rb') as f:
            torrent_content = base64.b64encode(f.read()).decode('utf-8')

        data = {
            'upload_type': 'torrent',
            'torrent_file': torrent_content,
            'anime_title': args.title,
            'subtitle_group': args.group,
            'season': args.season,
            'category': args.category
        }
        success = download_manager.process_manual_upload(data)
        if success:
            logger.info('âœ… Torrentæ–‡ä»¶å¤„ç†æˆåŠŸ')
        else:
            logger.error('âŒ Torrentæ–‡ä»¶å¤„ç†å¤±è´¥')
    except Exception as e:
        logger.error(f'âŒ è¯»å–Torrentæ–‡ä»¶å¤±è´¥: {e}')


def start_webhook_server(host: str, port: int):
    """
    å¯åŠ¨ Webhook æœåŠ¡å™¨ã€‚

    Args:
        host: ç›‘å¬åœ°å€
        port: ç›‘å¬ç«¯å£
    """
    from flask import Flask
    from src.interface.webhook.handler import create_webhook_blueprint

    app = Flask(__name__)
    app.register_blueprint(create_webhook_blueprint())

    # ä½¿ç”¨ Werkzeug é™é»˜æ¨¡å¼
    import logging as werkzeug_logging
    werkzeug_logging.getLogger('werkzeug').setLevel(werkzeug_logging.WARNING)

    app.run(host=host, port=port, debug=False, use_reloader=False)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    from src.core.config import config
    from src.container import container
    from src.services.ai_debug_service import ai_debug_service

    parser = argparse.ArgumentParser(description='AniDown - åŠ¨æ¼«ä¸‹è½½ç®¡ç†å™¨')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨debugæ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='è¿è¡ŒéªŒè¯æµ‹è¯•')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # RSSå‘½ä»¤
    rss_parser = subparsers.add_parser('rss', help='å¤„ç†RSSé“¾æ¥')
    rss_parser.add_argument('url', help='RSSé“¾æ¥')

    # ç£åŠ›é“¾æ¥å‘½ä»¤
    magnet_parser = subparsers.add_parser('magnet', help='å¤„ç†ç£åŠ›é“¾æ¥')
    magnet_parser.add_argument('hash', help='ç£åŠ›é“¾æ¥hash')
    magnet_parser.add_argument('title', help='åŠ¨æ¼«åç§°')
    magnet_parser.add_argument('group', help='å­—å¹•ç»„')
    magnet_parser.add_argument('--season', type=int, default=1, help='å­£æ•°')
    magnet_parser.add_argument('--category', default='tv', choices=['tv', 'movie'])

    # Torrentæ–‡ä»¶å‘½ä»¤
    torrent_parser = subparsers.add_parser('torrent', help='å¤„ç†Torrentæ–‡ä»¶')
    torrent_parser.add_argument('file', help='Torrentæ–‡ä»¶è·¯å¾„')
    torrent_parser.add_argument('title', help='åŠ¨æ¼«åç§°')
    torrent_parser.add_argument('group', help='å­—å¹•ç»„')
    torrent_parser.add_argument('--season', type=int, default=1, help='å­£æ•°')
    torrent_parser.add_argument('--category', default='tv', choices=['tv', 'movie'])

    args = parser.parse_args()

    # å¯ç”¨debugæ¨¡å¼
    if args.debug:
        ai_debug_service.enable()
        logger.info('ğŸ› DEBUGæ¨¡å¼å·²å¯ç”¨')
        logging.getLogger().setLevel(logging.DEBUG)

    # éªŒè¯æµ‹è¯•æ¨¡å¼
    if args.test:
        success = run_all_tests()
        sys.exit(0 if success else 1)

    logger.info('ğŸš€ AniDown å¯åŠ¨ä¸­...')
    logger.info(f'ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {os.getenv("CONFIG_PATH", "config.json")}')

    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()

    # æ¸…ç†ä¸Šæ¬¡è¿è¡Œé—ç•™çš„ processing çŠ¶æ€å†å²è®°å½•
    from src.infrastructure.repositories.history_repository import HistoryRepository
    history_repo = HistoryRepository()
    interrupted_count = history_repo.mark_processing_as_interrupted()
    if interrupted_count > 0:
        logger.info(f'ğŸ§¹ æ¸…ç†äº† {interrupted_count} æ¡ä¸Šæ¬¡è¿è¡Œé—ç•™çš„å¤„ç†ä¸­è®°å½•')

    # åˆå§‹åŒ– Discord Webhook
    init_discord_webhook()

    # åˆå§‹åŒ– API Key Pool
    init_key_pools()

    # è·å– DownloadManager å®ä¾‹
    download_manager = container.download_manager()

    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if args.command == 'rss':
        handle_rss_command(args, download_manager)
        return
    elif args.command == 'magnet':
        handle_magnet_command(args, download_manager)
        return
    elif args.command == 'torrent':
        handle_torrent_command(args, download_manager)
        return

    # é»˜è®¤å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼
    logger.info('ğŸ¬ å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼...')

    # å¯¼å…¥çŠ¶æ€ç®¡ç†å™¨
    from src.interface.web.controllers.system_status import system_status_manager

    # åˆå§‹åŒ–é˜Ÿåˆ—å·¥ä½œè€…
    webhook_queue, rss_queue = init_queue_workers(download_manager)

    # å¯åŠ¨ Webhook æœåŠ¡å™¨ (åå°çº¿ç¨‹)
    logger.info('ğŸ”— æ­£åœ¨å¯åŠ¨ Webhook æœåŠ¡å™¨...')
    logger.info(f'ğŸ“ Webhook åœ°å€: http://{config.webhook.host}:{config.webhook.port}')
    webhook_thread = Thread(
        target=start_webhook_server,
        kwargs={'host': config.webhook.host, 'port': config.webhook.port},
        daemon=True
    )
    webhook_thread.start()
    system_status_manager.set_webhook_status(True)
    logger.info('âœ… Webhook æœåŠ¡å™¨å·²åœ¨åå°å¯åŠ¨')

    # å¯åŠ¨ Web UI æœåŠ¡å™¨ (åå°çº¿ç¨‹)
    logger.info('ğŸŒ æ­£åœ¨å¯åŠ¨ Web UI æœåŠ¡å™¨...')

    def run_webui():
        from src.interface.web.app import create_app

        # ä½¿ç”¨ Werkzeug é™é»˜æ¨¡å¼
        import logging as werkzeug_logging
        werkzeug_logging.getLogger('werkzeug').setLevel(werkzeug_logging.WARNING)

        app = create_app(container)
        system_status_manager.set_webui_status(True)
        app.run(
            host=config.webui.host,
            port=config.webui.port,
            debug=False,
            use_reloader=False
        )

    webui_thread = Thread(target=run_webui, daemon=True)
    webui_thread.start()
    logger.info(f'âœ… Web UI æœåŠ¡å™¨å·²å¯åŠ¨: http://{config.webui.host}:{config.webui.port}')

    # å¯åŠ¨å®šæ—¶ä»»åŠ¡ (ä¸»çº¿ç¨‹)
    try:
        run_schedule(download_manager)
    except KeyboardInterrupt:
        logger.info('ğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...')
        system_status_manager.set_rss_scheduler_status(False)
        system_status_manager.set_webui_status(False)
        system_status_manager.set_webhook_status(False)

        # åœæ­¢é˜Ÿåˆ—å·¥ä½œè€…
        webhook_queue.stop()
        rss_queue.stop()

        # æ¸…ç†æœªå®Œæˆçš„ processing çŠ¶æ€å†å²è®°å½•
        from src.infrastructure.repositories.history_repository import HistoryRepository
        history_repo = HistoryRepository()
        interrupted_count = history_repo.mark_processing_as_interrupted()
        if interrupted_count > 0:
            logger.info(f'ğŸ§¹ æ ‡è®°äº† {interrupted_count} æ¡æœªå®Œæˆçš„è®°å½•ä¸ºå·²ä¸­æ–­')

        logger.info('âœ… å·²ä¼˜é›…å…³é—­')
    except Exception as e:
        logger.error(f'âŒ å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}', exc_info=True)
        system_status_manager.set_rss_scheduler_status(False)

        # æ¸…ç†æœªå®Œæˆçš„ processing çŠ¶æ€å†å²è®°å½•
        from src.infrastructure.repositories.history_repository import HistoryRepository
        history_repo = HistoryRepository()
        history_repo.mark_processing_as_interrupted()


if __name__ == '__main__':
    main()
